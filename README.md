# ğŸ‡³ğŸ‡µ Nepal Legal Mistral-7B

> **An end-to-end, domain-specific Large Language Model for Nepalese law**, engineered from the *National Penal Code of Nepal (2017)* â€” from raw legal PDFs to deployable, quantized AI systems.

This project demonstrates how a **country-specific legal LLM** can be built **faithfully, transparently, and reproducibly**, without relying on generic web data or opaque pipelines.

---

## ğŸ—ï¸ System Architecture

![System Architecture](https://github.com/yamrajkhadka/npc2017-legal-llm/blob/main/system-archi.png)

This diagram illustrates the **complete lifecycle** of the Nepal Legal LLM:
**official legal documents â†’ structured dataset â†’ fine-tuned model â†’ real-world deployments**.

---

## ğŸ” What This Project Is (and Is Not)

This is **not just a fine-tuned chatbot**.

It is a **full legal-LLM engineering pipeline**, designed to show how legally grounded AI systems can be created **from scratch**:

- ğŸ“„ Official government legal PDFs  
- ğŸ§¹ Faithful text extraction (no summaries, no hallucination)  
- ğŸ§© Hierarchical legal chunking (Part â†’ Chapter â†’ Section â†’ Subsection)  
- ğŸ·ï¸ Deterministic chunk IDs for traceability  
- ğŸ§  Instruction-tuning dataset generation  
- ğŸ”¥ Fine-tuning **Mistral-7B** for legal reasoning  
- âš¡ Quantization for low-resource inference  
- ğŸŒ Live deployments (UI + API)

The entire pipeline is **auditable, reproducible, and legally faithful**.

---

## ğŸš€ Live Systems

### ğŸ–¥ï¸ Interactive Web Apps

| Deployment | Description | Link |
|----------|------------|------|
| **Full-Precision Assistant** | Accurate legal reasoning | https://huggingface.co/spaces/yamraj047/penal-legal-assistant |
| **Fast API Version** | Optimized backend inference | https://huggingface.co/spaces/yamraj047/nepal-legal-assistant-fast |
| **GGUF Quantized Assistant** | Runs on low-RAM machines | https://huggingface.co/spaces/yamraj047/Nepall-legal-assist |

---

## ğŸ¤— Models

| Model | Format | Size | Link |
|------|-------|------|------|
| Nepal Legal Mistral-7B | FP16 | ~13.5 GB | https://huggingface.co/yamraj047/nepal-legal-mistral-7b |
| Nepal Legal Mistral-7B | GGUF (Q4_K_M) | **4.07 GB** | https://huggingface.co/yamraj047/nepal-legal-mistral-7b-GGUF |

---

## ğŸš€ Run Nepal Legal Mistral-7B Locally (FP16)

Run the **full-precision model locally** using Hugging Face `transformers`.

### ğŸ“¦ Requirements
- Python **3.9+**
- **16 GB RAM minimum** (CPU works, GPU optional)
- Disk space: **~14 GB**

---

### ğŸ”§ Installation

```bash
pip install transformers torch accelerate sentencepiece
â–¶ï¸ Run the Model (Interactive â€“ Terminal)
python3 -c "from transformers import pipeline; p=pipeline('text-generation','yamraj047/nepal-legal-mistral-7b'); print(p(input('Q: '), max_new_tokens=300)[0]['generated_text'])" ```

Example:
Q: Explain Article 20 of the Constitution of Nepal
Press Enter to receive the answer.
âš ï¸ First run will download ~13.5 GB and may be slow on CPU systems.
---
ğŸ’¡ Low-RAM Alternative (Recommended for Laptops)
If your system has limited memory, use the GGUF quantized model:
Size: 4.07 GB
Backend: llama.cpp
Runs efficiently on laptops and low-resource machines
Model link:
https://huggingface.co/yamraj047/nepal-legal-mistral-7b-GGUF
ğŸ§  Why This Matters
Most legal chatbots:
âŒ Are trained on generic web text
âŒ Ignore legal hierarchy
âŒ Cannot trace answers back to law
This system:
âœ… Preserves Nepalâ€™s legal structure
âœ… Grounds responses in exact legal sections
âœ… Reduces hallucination via chunk-level supervision
âœ… Runs on consumer hardware
---
ğŸ—‚ï¸ Repository Structure
.
â”œâ”€â”€ pdf-to-text/
â”‚   â”œâ”€â”€ pdf_to_text.py              # Faithful PDF â†’ text extraction
â”‚   â”œâ”€â”€ penal-english.pdf           # Official legal source
â”‚   â””â”€â”€ penal_code_raw.txt          # Clean extracted text
â”‚
â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ legal_chunking.py           # Hierarchy + chunk ID generation
â”‚   â””â”€â”€ penal_code_chunks.json
â”‚
â”œâ”€â”€ instruction-dataset/
â”‚   â””â”€â”€ npc_instruction_dataset.json
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ mistral_finetuning.ipynb    # Instruction fine-tuning
â”‚
â””â”€â”€ README.md
---
ğŸ§± Stage 1 â€” PDF â†’ Clean Text
Goal: Extract the law exactly as published.
No chunking
No summarization
No interpretation
This ensures legal authenticity.
---
ğŸ§© Stage 2 â€” Legal Chunking + Metadata
Each subsection becomes a single atomic legal unit:
{
  "law": "National Penal Code 2017",
  "part": "Part-1",
  "chapter": "Chapter-1",
  "section": 1,
  "subsection": "(1)",
  "chunk_id": "npc2017_p1_c1_s1_sub1"
}
This enables:
Traceable answers
Precise retrieval
Explainable AI outputs
---
ğŸ§  Stage 3 â€” Instruction Dataset Engineering
Instructions are systematically generated, not random:
Legal explanation
Scope & applicability
Classification questions
Negative examples ("not mentioned in law")
Each instruction retains full legal metadata.
---
ğŸ”¥ Stage 4 â€” Fine-Tuning
Base Model: Mistral-7B
Training Type: Instruction tuning
Focus: Legal understanding & reasoning
Result: a Nepal-specific legal LLM, not a generic chatbot.
---
âš¡ Stage 5 â€” Quantization
Metric	Value
Original size	13.5 GB
Quantized size	4.07 GB
Method	Q4_K_M
Compatible with	llama.cpp, LM Studio
---
ğŸŒ Stage 6 â€” Deployment
Gradio UI for public interaction
FastAPI backend for integration
GGUF inference for offline use
---
âš ï¸ Legal Disclaimer
This project is for research and educational purposes only.
â— Not a substitute for professional legal advice.
---
ğŸ‘¤ Author
Yamraj Khadka
Computer Engineering Undergraduate, Nepal ğŸ‡³ğŸ‡µ
ğŸ¤— Hugging Face: https://huggingface.co/yamraj047
ğŸ™ GitHub: https://github.com/yamrajkhadka
---
â­ Support the Project
If this project helped you:
â­ Star the repository
ğŸ´ Fork it
ğŸ§  Build on it
This work aims to raise the standard for Nepal-focused AI systems.

---
