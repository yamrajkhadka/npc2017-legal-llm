{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14500094,"sourceType":"datasetVersion","datasetId":9261518}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#/kaggle/input/kjhfdfdtty/nepal_legal_instruction_dataset-22.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:13:19.135634Z","iopub.execute_input":"2026-01-15T11:13:19.135954Z","iopub.status.idle":"2026-01-15T11:13:19.140936Z","shell.execute_reply.started":"2026-01-15T11:13:19.135918Z","shell.execute_reply":"2026-01-15T11:13:19.140151Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install required packages\n!pip install -q transformers datasets accelerate bitsandbytes peft trl torch\n!pip install -q sentencepiece protobuf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:13:19.142383Z","iopub.execute_input":"2026-01-15T11:13:19.142600Z","iopub.status.idle":"2026-01-15T11:13:29.316722Z","shell.execute_reply.started":"2026-01-15T11:13:19.142579Z","shell.execute_reply":"2026-01-15T11:13:29.315655Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import all required libraries\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"All libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:13:29.318412Z","iopub.execute_input":"2026-01-15T11:13:29.318742Z","iopub.status.idle":"2026-01-15T11:14:06.351241Z","shell.execute_reply.started":"2026-01-15T11:13:29.318705Z","shell.execute_reply":"2026-01-15T11:14:06.350555Z"}},"outputs":[{"name":"stderr","text":"2026-01-15 11:13:48.086918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768475628.274994      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768475628.334355      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768475628.774520      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768475628.774560      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768475628.774563      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768475628.774565      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"All libraries imported successfully!\nPyTorch version: 2.8.0+cu126\nCUDA available: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# SUPER OPTIMIZED Configuration\nMODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nOUTPUT_DIR = \"./nepal-legal-model\"\nMAX_LENGTH = 256  # Much shorter = MUCH faster\nBATCH_SIZE = 4    # Larger batches\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 2    # Fewer epochs to start\nDATA_PATH = \"/kaggle/input/kjhfdfdtty/nepal_legal_instruction_dataset-22.json\"\n\nprint(\"‚ö° OPTIMIZED FOR SPEED:\")\nprint(f\"  Max Length: {MAX_LENGTH}\")\nprint(f\"  Batch Size: {BATCH_SIZE}\")\nprint(f\"  Expected time: 45-60 min per epoch\")\nprint(f\"  Total for 2 epochs: ~2 hours\")\n\nwith open(DATA_PATH, 'r', encoding='utf-8') as f:\n    data = json.load(f)\nprint(f\"‚úì Loaded {len(data)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:14:06.352087Z","iopub.execute_input":"2026-01-15T11:14:06.352349Z","iopub.status.idle":"2026-01-15T11:14:06.573189Z","shell.execute_reply.started":"2026-01-15T11:14:06.352327Z","shell.execute_reply":"2026-01-15T11:14:06.572543Z"}},"outputs":[{"name":"stdout","text":"‚ö° OPTIMIZED FOR SPEED:\n  Max Length: 256\n  Batch Size: 4\n  Expected time: 45-60 min per epoch\n  Total for 2 epochs: ~2 hours\n‚úì Loaded 5101 examples\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# First, let's check the structure of your data\nprint(\"Checking data structure...\")\nprint(f\"Type of data: {type(data)}\")\nprint(f\"Type of first element: {type(data[0])}\")\n\n# Flatten the data if it's nested\nflattened_data = []\nfor item in data:\n    if isinstance(item, list):\n        # If item is a list, extend it\n        flattened_data.extend(item)\n    elif isinstance(item, dict):\n        # If item is a dict, append it\n        flattened_data.append(item)\n    else:\n        print(f\"Warning: Unexpected data type: {type(item)}\")\n\nprint(f\"Flattened to {len(flattened_data)} examples\")\nprint(f\"\\nFirst example structure:\")\nprint(json.dumps(flattened_data[0], indent=2)[:500])\n\n# Format the data into instruction-following format\ndef format_instruction(sample):\n    \"\"\"Format the data into instruction-following format\"\"\"\n    instruction = sample.get('instruction', '')\n    input_text = sample.get('input', '')\n    output = sample.get('output', '')\n    \n    # Create prompt in instruction format\n    if input_text:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Response:\n{output}\"\"\"\n    \n    return {\"text\": prompt}\n\n# Convert to Dataset format\nprint(\"\\nConverting data to Dataset format...\")\ndataset = Dataset.from_list(flattened_data)\n\n# Apply formatting\nprint(\"Formatting data...\")\ndataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n\n# Split into train and validation\nprint(\"Splitting into train and validation...\")\ndataset = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"test\"]\n\nprint(f\"\\n‚úì Training samples: {len(train_dataset)}\")\nprint(f\"‚úì Validation samples: {len(eval_dataset)}\")\nprint(f\"\\nSample formatted text:\")\nprint(\"-\" * 80)\nprint(train_dataset[0][\"text\"][:500])\nprint(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:14:06.574229Z","iopub.execute_input":"2026-01-15T11:14:06.574669Z","iopub.status.idle":"2026-01-15T11:14:07.136141Z","shell.execute_reply.started":"2026-01-15T11:14:06.574647Z","shell.execute_reply":"2026-01-15T11:14:07.135579Z"}},"outputs":[{"name":"stdout","text":"Checking data structure...\nType of data: <class 'list'>\nType of first element: <class 'dict'>\nFlattened to 6400 examples\n\nFirst example structure:\n{\n  \"instruction\": \"Explain the scope and jurisdiction of this law.\",\n  \"input\": \"This Act may be cited as the 'National Penal (Code) Act, 2017'.\",\n  \"output\": \"National Penal Code 2017 is a law of Nepal that sets out the penal provisions of the country. This Act defines various crimes and their punishments, as well as the procedures for investigation, trial, and appeal. The law applies to all offenses committed within the territory of Nepal, and it provides for the establishment of a penal syst\n\nConverting data to Dataset format...\nFormatting data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b5c337827744c490033720e6a57871"}},"metadata":{}},{"name":"stdout","text":"Splitting into train and validation...\n\n‚úì Training samples: 5760\n‚úì Validation samples: 640\n\nSample formatted text:\n--------------------------------------------------------------------------------\n### Instruction:\nExplain the legal meaning of this provision in simple language.\n\n### Input:\nHow does the provision on 'Prohibition of obstructing public servant' apply to situations in Nepal under National Penal Code 2017?\n\n### Response:\nprovision on 'Prohibition of obstructing public servant' in Nepal's National Penal Code 2017 is meant to prevent people from hindering or blocking the work of public servants. This provision applies to any person who intentionally obstructs or hinders a public \n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Configure quantization for memory efficiency\nprint(\"Setting up quantization config...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"‚úì Quantization config ready\")\n\n# Load tokenizer\nprint(f\"\\nLoading tokenizer for {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"‚úì Tokenizer loaded successfully\")\n\n# Load model with quantization\nprint(f\"\\nLoading model {MODEL_NAME}...\")\nprint(\"This may take 2-3 minutes...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(\"‚úì Model loaded successfully!\")\nprint(f\"Model device: {model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:14:07.137134Z","iopub.execute_input":"2026-01-15T11:14:07.137433Z","iopub.status.idle":"2026-01-15T11:15:33.516555Z","shell.execute_reply.started":"2026-01-15T11:14:07.137408Z","shell.execute_reply":"2026-01-15T11:15:33.515868Z"}},"outputs":[{"name":"stdout","text":"Setting up quantization config...\n‚úì Quantization config ready\n\nLoading tokenizer for mistralai/Mistral-7B-v0.1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1fc316645e4f0faab40881b7215893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a765fb4e2e4e6aa6f4e6c0fe295417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a99edee7904f1ca8ca931fedacd914"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21e623e6a094fd986312944697bb7f6"}},"metadata":{}},{"name":"stdout","text":"‚úì Tokenizer loaded successfully\n\nLoading model mistralai/Mistral-7B-v0.1...\nThis may take 2-3 minutes...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a2814403f84cefae01406aa496ef69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6771f4f2444c416791ba3808e91742ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e45122f0e77496fafb1680ab1172e5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4fa25f3c31a4ca093aaf89594966185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d4201f358a4bb197c7695c7ccd61da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5140bd60d57e44a5bb14c800682d449f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde21470f6d64efc92865f86ee6fba71"}},"metadata":{}},{"name":"stdout","text":"‚úì Model loaded successfully!\nModel device: cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Prepare model for k-bit training\nprint(\"Preparing model for training...\")\nmodel = prepare_model_for_kbit_training(model)\n\nprint(\"‚úì Model prepared for k-bit training\")\n\n# Configure LoRA\nprint(\"\\nConfiguring LoRA...\")\nlora_config = LoraConfig(\n    r=16,  # Rank\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nprint(\"‚úì LoRA config created\")\n\n# Apply LoRA to model\nprint(\"\\nApplying LoRA to model...\")\nmodel = get_peft_model(model, lora_config)\n\nprint(\"‚úì LoRA applied successfully!\")\n\n# Print trainable parameters\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"\\nüìä Trainable params: {trainable_params:,} || \"\n        f\"All params: {all_param:,} || \"\n        f\"Trainable%: {100 * trainable_params / all_param:.2f}%\"\n    )\n\nprint_trainable_parameters(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:15:33.518405Z","iopub.execute_input":"2026-01-15T11:15:33.519207Z","iopub.status.idle":"2026-01-15T11:15:34.201317Z","shell.execute_reply.started":"2026-01-15T11:15:33.519181Z","shell.execute_reply":"2026-01-15T11:15:34.200587Z"}},"outputs":[{"name":"stdout","text":"Preparing model for training...\n‚úì Model prepared for k-bit training\n\nConfiguring LoRA...\n‚úì LoRA config created\n\nApplying LoRA to model...\n‚úì LoRA applied successfully!\n\nüìä Trainable params: 41,943,040 || All params: 3,794,014,208 || Trainable%: 1.11%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Configure training arguments\nprint(\"Setting up training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    max_grad_norm=0.3,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n)\n\nprint(\"‚úì Training arguments configured!\")\nprint(f\"\\nüìã Training Configuration:\")\nprint(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\nprint(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\nprint(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  ‚Ä¢ Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\nprint(f\"  ‚Ä¢ Max length: {MAX_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:15:34.202206Z","iopub.execute_input":"2026-01-15T11:15:34.202467Z","iopub.status.idle":"2026-01-15T11:15:34.239146Z","shell.execute_reply.started":"2026-01-15T11:15:34.202446Z","shell.execute_reply":"2026-01-15T11:15:34.238377Z"}},"outputs":[{"name":"stdout","text":"Setting up training arguments...\n‚úì Training arguments configured!\n\nüìã Training Configuration:\n  ‚Ä¢ Epochs: 2\n  ‚Ä¢ Batch size: 4\n  ‚Ä¢ Gradient accumulation: 4\n  ‚Ä¢ Effective batch size: 16\n  ‚Ä¢ Learning rate: 0.0002\n  ‚Ä¢ Max length: 256\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import Trainer, DataCollatorForLanguageModeling\n\nprint(\"Setting up trainer...\")\n\n# Tokenize the datasets\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding=\"max_length\",\n    )\n\nprint(\"Tokenizing datasets...\")\ntokenized_train = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)\n\ntokenized_eval = eval_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=eval_dataset.column_names\n)\n\nprint(\"‚úì Datasets tokenized\")\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_eval,\n    data_collator=data_collator,\n)\n\nprint(\"‚úì Trainer created successfully!\")\nprint(f\"\\nüéØ Ready to train on {len(train_dataset)} examples\")\nprint(f\"üìä Total training steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:15:34.240245Z","iopub.execute_input":"2026-01-15T11:15:34.240916Z","iopub.status.idle":"2026-01-15T11:15:36.781241Z","shell.execute_reply.started":"2026-01-15T11:15:34.240868Z","shell.execute_reply":"2026-01-15T11:15:36.780465Z"}},"outputs":[{"name":"stdout","text":"Setting up trainer...\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc46070c2a6a495c87ef0a522ecc36fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/640 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"168f52f3264640ab92b744788abfe367"}},"metadata":{}},{"name":"stdout","text":"‚úì Datasets tokenized\n‚úì Trainer created successfully!\n\nüéØ Ready to train on 5760 examples\nüìä Total training steps: 720\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Start training\nprint(\"=\" * 80)\nprint(\"üöÄ STARTING TRAINING\")\nprint(\"=\" * 80)\nprint(f\"\\nTraining on {len(train_dataset)} examples for {NUM_EPOCHS} epochs\")\nprint(f\"This will take approximately 30-60 minutes per epoch\\n\")\nprint(\"You can monitor the progress below:\")\nprint(\"-\" * 80)\n\n# Train the model\ntrainer.train()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ TRAINING COMPLETE!\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T11:15:36.782209Z","iopub.execute_input":"2026-01-15T11:15:36.782471Z","iopub.status.idle":"2026-01-15T17:12:52.376903Z","shell.execute_reply.started":"2026-01-15T11:15:36.782436Z","shell.execute_reply":"2026-01-15T17:12:52.376249Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüöÄ STARTING TRAINING\n================================================================================\n\nTraining on 5760 examples for 2 epochs\nThis will take approximately 30-60 minutes per epoch\n\nYou can monitor the progress below:\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 5:56:45, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.417300</td>\n      <td>0.413377</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.260600</td>\n      <td>0.390246</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n================================================================================\n‚úÖ TRAINING COMPLETE!\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Save the fine-tuned model - FIXED VERSION WITH MEMORY MANAGEMENT\nprint(\"Saving the fine-tuned model...\")\n\n# Save the LoRA adapter\ntrainer.model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(f\"‚úì Model saved to {OUTPUT_DIR}\")\n\n# CRITICAL: Free up GPU memory before merging\nprint(\"\\nFreeing GPU memory...\")\ndel model\ndel trainer\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"‚úì Memory cleared\")\n\n# Merge LoRA weights with base model for easier deployment\nprint(\"\\nMerging LoRA weights with base model...\")\n\nfrom peft import PeftModel\n\n# Load base model in fp16 (not quantized) for merging\nprint(\"Loading base model in fp16...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\n\n# Load and merge LoRA weights\nprint(\"Loading LoRA adapter...\")\nmerged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n\nprint(\"Merging weights...\")\nmerged_model = merged_model.merge_and_unload()\n\n# Save merged model\nMERGED_OUTPUT_DIR = \"./nepal-legal-model-merged\"\nprint(f\"Saving merged model to {MERGED_OUTPUT_DIR}...\")\nmerged_model.save_pretrained(MERGED_OUTPUT_DIR)\ntokenizer.save_pretrained(MERGED_OUTPUT_DIR)\n\nprint(f\"‚úì Merged model saved to {MERGED_OUTPUT_DIR}\")\nprint(\"\\n‚úÖ All models saved successfully!\")\n\n# Clear memory again\ndel merged_model\ndel base_model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:12:52.381379Z","iopub.execute_input":"2026-01-15T17:12:52.381590Z","iopub.status.idle":"2026-01-15T17:14:05.497286Z","shell.execute_reply.started":"2026-01-15T17:12:52.381570Z","shell.execute_reply":"2026-01-15T17:14:05.496645Z"}},"outputs":[{"name":"stdout","text":"Saving the fine-tuned model...\n‚úì Model saved to ./nepal-legal-model\n\nFreeing GPU memory...\n‚úì Memory cleared\n\nMerging LoRA weights with base model...\nLoading base model in fp16...\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60e273fda7e45918ff9ebb5c9f4c8b2"}},"metadata":{}},{"name":"stdout","text":"Loading LoRA adapter...\nMerging weights...\nSaving merged model to ./nepal-legal-model-merged...\n‚úì Merged model saved to ./nepal-legal-model-merged\n\n‚úÖ All models saved successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Test the fine-tuned model\nprint(\"Testing the fine-tuned model...\\n\")\n\n# Load the merged model for inference\ntest_model = AutoModelForCausalLM.from_pretrained(\n    MERGED_OUTPUT_DIR,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\ntest_tokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT_DIR)\n\nprint(\"‚úì Model loaded for testing\\n\")\n\n# Test examples about Nepal legal system\ntest_cases = [\n    {\n        \"instruction\": \"Explain the legal meaning of this provision in simple language.\",\n        \"input\": \"What is the punishment for theft under National Penal Code 2017?\"\n    },\n    {\n        \"instruction\": \"Summarize this legal provision.\",\n        \"input\": \"Explain the scope and jurisdiction of National Penal Code 2017.\"\n    },\n    {\n        \"instruction\": \"What are the key elements of this offense?\",\n        \"input\": \"Describe the offense of obstructing a public servant.\"\n    }\n]\n\n# Run inference\nfor i, test_case in enumerate(test_cases, 1):\n    print(f\"{'='*80}\")\n    print(f\"TEST CASE {i}\")\n    print(f\"{'='*80}\")\n    \n    # Format the prompt\n    if test_case.get(\"input\"):\n        prompt = f\"\"\"### Instruction:\n{test_case['instruction']}\n\n### Input:\n{test_case['input']}\n\n### Response:\n\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{test_case['instruction']}\n\n### Response:\n\"\"\"\n    \n    print(f\"PROMPT:\\n{prompt}\")\n    print(f\"\\nGENERATED RESPONSE:\")\n    print(\"-\" * 80)\n    \n    # Generate response\n    inputs = test_tokenizer(prompt, return_tensors=\"pt\").to(test_model.device)\n    outputs = test_model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True,\n        pad_token_id=test_tokenizer.eos_token_id\n    )\n    \n    response = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract only the response part\n    response = response.split(\"### Response:\")[-1].strip()\n    \n    print(response)\n    print(f\"\\n\")\n\nprint(\"‚úÖ Testing complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom datetime import datetime\nimport shutil\n\nprint(\"=\"*80)\nprint(\"üì¶ PREPARING FILES FOR DOWNLOAD (No ZIP - Direct Download)\")\nprint(\"=\"*80)\n\n# Clean up any existing zips to free space\nprint(\"\\nüßπ Cleaning up space...\")\nfor f in os.listdir('.'):\n    if f.endswith('.zip'):\n        try:\n            os.remove(f)\n            print(f\"   Removed: {f}\")\n        except:\n            pass\n\n# Check what we have\nprint(\"\\nüìÇ Checking available models...\")\nmerged_exists = os.path.exists(MERGED_OUTPUT_DIR)\nlora_exists = os.path.exists(OUTPUT_DIR)\n\nprint(f\"   {'‚úì' if merged_exists else '‚úó'} Merged model: {MERGED_OUTPUT_DIR}\")\nprint(f\"   {'‚úì' if lora_exists else '‚úó'} LoRA adapter: {OUTPUT_DIR}\")\n\n# Save configuration and usage files\nprint(\"\\nüìù Creating documentation files...\")\n\n# 1. Training configuration\nconfig = {\n    \"model_info\": {\n        \"base_model\": MODEL_NAME,\n        \"model_type\": \"Mistral-7B Fine-tuned for Nepal Legal System\",\n        \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    },\n    \"hyperparameters\": {\n        \"max_length\": MAX_LENGTH,\n        \"batch_size\": BATCH_SIZE,\n        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n        \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n        \"learning_rate\": LEARNING_RATE,\n        \"num_epochs\": NUM_EPOCHS,\n        \"lora_r\": 16,\n        \"lora_alpha\": 32,\n        \"lora_dropout\": 0.05,\n    },\n    \"dataset\": {\n        \"source\": \"Nepal Legal Instruction Dataset\",\n        \"train_samples\": len(train_dataset) if 'train_dataset' in globals() else \"N/A\",\n        \"eval_samples\": len(eval_dataset) if 'eval_dataset' in globals() else \"N/A\",\n    },\n    \"files_included\": {\n        \"merged_model\": merged_exists,\n        \"lora_adapter\": lora_exists,\n    }\n}\n\nwith open('MODEL_INFO.json', 'w') as f:\n    json.dump(config, f, indent=2)\nprint(\"   ‚úì MODEL_INFO.json\")\n\n# 2. Quick start guide\nquick_start = \"\"\"# Nepal Legal AI Model - Quick Start Guide\n\n## ‚ö° Fastest Way to Use\n\n### Option 1: Use Merged Model (Recommended - Easiest)\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model (folder: nepal-legal-model-merged)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./nepal-legal-model-merged\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./nepal-legal-model-merged\")\n\n# Ask a question\nprompt = \\\"\\\"\\\"### Instruction:\nExplain this in simple terms.\n\n### Input:\nWhat is theft under Nepal law?\n\n### Response:\n\\\"\\\"\\\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=200)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n### Option 2: Use LoRA Adapter (Smaller Download)\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# First download base model from Hugging Face\nbase = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# Load your adapter (folder: nepal-legal-model)\nmodel = PeftModel.from_pretrained(base, \"./nepal-legal-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./nepal-legal-model\")\n```\n\n## üì¶ What to Download\n\n**For immediate use:**\n- Download the entire `nepal-legal-model-merged` folder (~14GB)\n- This is ready to use, no other downloads needed\n\n**For smaller download:**\n- Download the `nepal-legal-model` folder (~150MB)\n- You'll need to download Mistral-7B base model separately\n\n## üíª System Requirements\n\n- Python 3.8+\n- 16GB RAM (8GB minimum with quantization)\n- GPU optional but recommended (8GB+ VRAM)\n\n## üì• Installation\n\n```bash\npip install transformers torch peft accelerate\n```\n\n## ‚ö†Ô∏è Important Notes\n\nThis model provides legal information but is NOT a substitute for professional legal advice.\nAlways consult qualified legal professionals for legal matters.\n\n## üîó Model Details\n\n- Base: Mistral-7B-v0.1\n- Dataset: 6,400 Nepal legal Q&A pairs\n- Focus: National Penal Code 2017\n- Language: English\n\n---\nCreated: \"\"\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nwith open('QUICK_START.md', 'w') as f:\n    f.write(quick_start)\nprint(\"   ‚úì QUICK_START.md\")\n\n# 3. Example usage script\nexample = \"\"\"#!/usr/bin/env python3\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./nepal-legal-model-merged\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./nepal-legal-model-merged\")\nprint(\"Model loaded!\\\\n\")\n\n# Example questions\nquestions = [\n    \"What is theft under Nepal law?\",\n    \"What is the punishment for assault?\",\n    \"Explain obstruction of public servant offense.\"\n]\n\nfor q in questions:\n    prompt = f\\\"\\\"\\\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{q}\n\n### Response:\n\\\"\\\"\\\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = answer.split(\"### Response:\")[-1].strip()\n    \n    print(f\"Q: {q}\")\n    print(f\"A: {answer}\\\\n\")\n    print(\"-\" * 80)\n\"\"\"\n\nwith open('example_usage.py', 'w') as f:\n    f.write(example)\nprint(\"   ‚úì example_usage.py\")\n\n# 4. Download instructions\ndownload_guide = \"\"\"# üì• How to Download Your Model\n\n## In Kaggle Notebook\n\n### Method 1: Using Kaggle UI (Easiest)\n1. Look at the RIGHT SIDEBAR ‚Üí Find \"Output\" section\n2. Or click \"Output Files\" at the top right\n3. You'll see folders listed:\n   - `nepal-legal-model-merged/` (14GB - Full model)\n   - `nepal-legal-model/` (150MB - LoRA adapter)\n4. Click the download icon next to each folder\n\n### Method 2: Using Kaggle API\n```bash\n# Install Kaggle\npip install kaggle\n\n# Download output\nkaggle kernels output <your-username>/<kernel-name> -p ./download\n```\n\n### Method 3: Direct Download (If Published)\nIf you've made the notebook public, share the URL and others can download outputs.\n\n## After Download\n\n1. **Using Merged Model:**\n   - Extract `nepal-legal-model-merged` folder\n   - Run `python example_usage.py`\n\n2. **Using LoRA Adapter:**\n   - Extract `nepal-legal-model` folder\n   - Download Mistral-7B base model\n   - Load adapter on top of base model\n\n## File Sizes\n\n- **Merged Model:** ~14GB (everything included)\n- **LoRA Adapter:** ~150MB (needs base model)\n- **Documentation:** <1MB\n\n## Recommended Download\n\nFor most users: Download `nepal-legal-model-merged`\n- It's ready to use immediately\n- No additional downloads needed\n- Easier to set up\n\n## Upload to Hugging Face (Optional)\n\nTo share your model:\n```python\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\nmodel.push_to_hub(\"your-username/nepal-legal-mistral\")\ntokenizer.push_to_hub(\"your-username/nepal-legal-mistral\")\n```\n\nThen anyone can use:\n```python\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"your-username/nepal-legal-mistral\")\n```\n\"\"\"\n\nwith open('DOWNLOAD_GUIDE.md', 'w') as f:\n    f.write(download_guide)\nprint(\"   ‚úì DOWNLOAD_GUIDE.md\")\n\n# Get folder sizes\ndef get_folder_size(path):\n    total = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                if os.path.exists(fp):\n                    total += os.path.getsize(fp)\n    except:\n        pass\n    return total\n\n# Summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ EVERYTHING READY FOR DOWNLOAD!\")\nprint(\"=\"*80)\n\nprint(\"\\nüìÇ Available Folders to Download:\\n\")\n\nif merged_exists:\n    size_gb = get_folder_size(MERGED_OUTPUT_DIR) / (1024**3)\n    print(f\"‚úì {MERGED_OUTPUT_DIR}/ ({size_gb:.2f} GB)\")\n    print(\"  ‚îî‚îÄ Full merged model - Ready to use!\")\n\nif lora_exists:\n    size_mb = get_folder_size(OUTPUT_DIR) / (1024**2)\n    print(f\"\\n‚úì {OUTPUT_DIR}/ ({size_mb:.1f} MB)\")\n    print(\"  ‚îî‚îÄ LoRA adapter only - Smaller download\")\n\nprint(\"\\nüìÑ Documentation Files:\")\nprint(\"  ‚úì MODEL_INFO.json - Training configuration\")\nprint(\"  ‚úì QUICK_START.md - How to use the model\")\nprint(\"  ‚úì example_usage.py - Example Python script\")\nprint(\"  ‚úì DOWNLOAD_GUIDE.md - Download instructions\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì• HOW TO DOWNLOAD\")\nprint(\"=\"*80)\nprint(\"\\n1. Click 'Output Files' at the top right of Kaggle\")\nprint(\"2. Find the folders listed above\")\nprint(\"3. Click the download button next to each folder\")\nprint(\"4. Download documentation files too\")\n\nprint(\"\\nüí° RECOMMENDATION:\")\nif merged_exists:\n    print(\"   Download 'nepal-legal-model-merged' folder\")\n    print(\"   ‚Üí It's the complete model, ready to use immediately\")\nelse:\n    print(\"   Download 'nepal-legal-model' folder (LoRA adapter)\")\n    print(\"   ‚Üí Smaller, but requires base Mistral-7B model\")\n\nprint(\"\\n‚úÖ All files ready! Check the Output section to download.\")\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:14:05.500307Z","iopub.execute_input":"2026-01-15T17:14:05.500643Z","iopub.status.idle":"2026-01-15T17:14:05.526413Z","shell.execute_reply.started":"2026-01-15T17:14:05.500609Z","shell.execute_reply":"2026-01-15T17:14:05.525809Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüì¶ PREPARING FILES FOR DOWNLOAD (No ZIP - Direct Download)\n================================================================================\n\nüßπ Cleaning up space...\n\nüìÇ Checking available models...\n   ‚úì Merged model: ./nepal-legal-model-merged\n   ‚úì LoRA adapter: ./nepal-legal-model\n\nüìù Creating documentation files...\n   ‚úì MODEL_INFO.json\n   ‚úì QUICK_START.md\n   ‚úì example_usage.py\n   ‚úì DOWNLOAD_GUIDE.md\n\n================================================================================\n‚úÖ EVERYTHING READY FOR DOWNLOAD!\n================================================================================\n\nüìÇ Available Folders to Download:\n\n‚úì ./nepal-legal-model-merged/ (13.49 GB)\n  ‚îî‚îÄ Full merged model - Ready to use!\n\n‚úì ./nepal-legal-model/ (655.2 MB)\n  ‚îî‚îÄ LoRA adapter only - Smaller download\n\nüìÑ Documentation Files:\n  ‚úì MODEL_INFO.json - Training configuration\n  ‚úì QUICK_START.md - How to use the model\n  ‚úì example_usage.py - Example Python script\n  ‚úì DOWNLOAD_GUIDE.md - Download instructions\n\n================================================================================\nüì• HOW TO DOWNLOAD\n================================================================================\n\n1. Click 'Output Files' at the top right of Kaggle\n2. Find the folders listed above\n3. Click the download button next to each folder\n4. Download documentation files too\n\nüí° RECOMMENDATION:\n   Download 'nepal-legal-model-merged' folder\n   ‚Üí It's the complete model, ready to use immediately\n\n‚úÖ All files ready! Check the Output section to download.\n\n================================================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\"\"\"\nMEMORY-EFFICIENT Upload to HuggingFace\nUploads files directly without loading the full model into RAM\n\"\"\"\n\nfrom huggingface_hub import HfApi, create_repo, whoami, login\nimport os\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nHF_TOKEN = \"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\"  # ‚Üê YOUR TOKEN HERE\n\n# Path to your model\nMODEL_PATH = \"./nepal-legal-model-merged\"\nMODEL_NAME = \"nepal-legal-mistral-7b\"\n\n# ============================================================================\n# STEP 1: LOGIN\n# ============================================================================\n\nprint(\"üîë Logging in to HuggingFace...\")\ntry:\n    login(token=HF_TOKEN, add_to_git_credential=True)\n    user_info = whoami(token=HF_TOKEN)\n    username = user_info['name']\n    print(f\"‚úÖ Logged in as: {username}\")\nexcept Exception as e:\n    print(f\"‚ùå Login failed: {e}\")\n    exit(1)\n\nREPO_ID = f\"{username}/{MODEL_NAME}\"\n\n# ============================================================================\n# STEP 2: CREATE REPOSITORY\n# ============================================================================\n\nprint(f\"\\nüì¶ Creating repository: {REPO_ID}\")\ntry:\n    repo_url = create_repo(\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        private=False,\n        exist_ok=True\n    )\n    print(f\"‚úÖ Repository ready: https://huggingface.co/{REPO_ID}\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to create repo: {e}\")\n    exit(1)\n\n# ============================================================================\n# STEP 3: UPLOAD FILES DIRECTLY (NO MODEL LOADING!)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üöÄ UPLOADING FILES\")\nprint(\"=\"*80)\nprint(\"‚è±Ô∏è  This will take 20-40 minutes depending on your connection\")\nprint(\"üí° You can close this tab - Kaggle will continue in background\")\nprint(\"=\"*80 + \"\\n\")\n\napi = HfApi()\n\ntry:\n    # Upload the entire folder\n    # This uploads files one by one WITHOUT loading them into RAM!\n    print(f\"üì§ Uploading folder: {MODEL_PATH}\")\n    print(\"   Files will be uploaded one at a time to save memory...\\n\")\n    \n    api.upload_folder(\n        folder_path=MODEL_PATH,\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        repo_type=\"model\",\n        commit_message=\"Upload Nepal Legal Mistral 7B model\",\n        multi_commits=True,  # Split into multiple commits to save memory\n        multi_commits_verbose=True  # Show progress\n    )\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üéâ UPLOAD COMPLETE!\")\n    print(\"=\"*80)\n    print(f\"\\nüîó Your model: https://huggingface.co/{REPO_ID}\")\n    print(f\"\\nüíª Usage:\")\n    print(f\"\"\"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{REPO_ID}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{REPO_ID}\")\n\"\"\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå Upload failed: {e}\")\n    print(\"\\nüîç Troubleshooting:\")\n    print(\"  1. Check internet connection\")\n    print(\"  2. Verify files exist in:\", MODEL_PATH)\n    print(\"  3. Check Kaggle hasn't timed out (max 9 hours)\")\n\n# ============================================================================\n# STEP 4: CREATE MODEL CARD\n# ============================================================================\n\nprint(\"\\nüìù Creating model card...\")\n\nmodel_card_content = f\"\"\"---\nlanguage:\n- ne\n- en\nlicense: apache-2.0\ntags:\n- legal\n- nepal\n- mistral\n- fine-tuned\n- nepali\nbase_model: mistralai/Mistral-7B-v0.1\nlibrary_name: transformers\n---\n\n# Nepal Legal Mistral 7B\n\nA Mistral 7B model fine-tuned on Nepali legal documents and case law.\n\n## Model Description\n\nFine-tuned using LoRA on Nepali legal texts including Supreme Court decisions, legal codes, and constitutional documents.\n\n## Usage\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{REPO_ID}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{REPO_ID}\")\n\nprompt = \"‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®‡§Æ‡§æ ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§π‡§ï‡§π‡§∞‡•Ç ‡§ï‡•á ‡§ï‡•á ‡§õ‡§®‡•ç?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=512)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Training Details\n\n- **Base Model:** Mistral-7B-v0.1\n- **Method:** LoRA (Low-Rank Adaptation)\n- **Languages:** Nepali, English\n- **Domain:** Legal/Judicial\n\n## Limitations\n\n‚ö†Ô∏è **Not a substitute for professional legal advice.** For educational and research purposes only.\n\n## License\n\nApache 2.0\n\"\"\"\n\ntry:\n    # Upload README as a separate file\n    api.upload_file(\n        path_or_fileobj=model_card_content.encode(),\n        path_in_repo=\"README.md\",\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        commit_message=\"Add model card\"\n    )\n    print(\"‚úÖ Model card uploaded\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Could not upload model card: {e}\")\n\nprint(\"\\n‚ú® All done!\")\nprint(f\"üîó View your model: https://huggingface.co/{REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:14:05.527299Z","iopub.execute_input":"2026-01-15T17:14:05.527499Z","iopub.status.idle":"2026-01-15T17:14:10.189091Z","shell.execute_reply.started":"2026-01-15T17:14:05.527481Z","shell.execute_reply":"2026-01-15T17:14:10.188273Z"}},"outputs":[{"name":"stdout","text":"üîë Logging in to HuggingFace...\n","output_type":"stream"},{"name":"stderr","text":"Token has not been saved to git credential helper.\nWARNING:huggingface_hub._login:Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n‚úÖ Logged in as: yamraj047\n\nüì¶ Creating repository: yamraj047/nepal-legal-mistral-7b\n‚úÖ Repository ready: https://huggingface.co/yamraj047/nepal-legal-mistral-7b\n\n================================================================================\nüöÄ UPLOADING FILES\n================================================================================\n‚è±Ô∏è  This will take 20-40 minutes depending on your connection\nüí° You can close this tab - Kaggle will continue in background\n================================================================================\n\nüì§ Uploading folder: ./nepal-legal-model-merged\n   Files will be uploaded one at a time to save memory...\n\n\n‚ùå Upload failed: HfApi.upload_folder() got an unexpected keyword argument 'multi_commits'\n\nüîç Troubleshooting:\n  1. Check internet connection\n  2. Verify files exist in: ./nepal-legal-model-merged\n  3. Check Kaggle hasn't timed out (max 9 hours)\n\nüìù Creating model card...\n‚úÖ Model card uploaded\n\n‚ú® All done!\nüîó View your model: https://huggingface.co/yamraj047/nepal-legal-mistral-7b\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\"\"\"\nFINAL: Upload Nepal Legal Model to HuggingFace\nCopy this entire cell to your Kaggle notebook\n\"\"\"\n\nfrom huggingface_hub import HfApi, create_repo, whoami, login\nimport os\n\nprint(\"=\"*80)\nprint(\"üöÄ UPLOADING TO HUGGINGFACE\")\nprint(\"=\"*80)\n\n# ============================================================================\n# STEP 1: PASTE YOUR TOKEN HERE\n# ============================================================================\n# Get your token from: https://huggingface.co/settings/tokens\n# Make sure it has WRITE permission!\n\nHF_TOKEN = \"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\"  # ‚Üê PASTE YOUR TOKEN HERE\n\n# ============================================================================\n# STEP 2: LOGIN\n# ============================================================================\n\nprint(\"\\nüîë Logging in to HuggingFace...\")\ntry:\n    login(token=HF_TOKEN, add_to_git_credential=True)\n    user_info = whoami(token=HF_TOKEN)\n    username = user_info['name']\n    print(f\"‚úÖ Success! Logged in as: {username}\")\nexcept Exception as e:\n    print(f\"‚ùå Login failed: {e}\")\n    print(\"\\n‚ö†Ô∏è  Common issues:\")\n    print(\"   1. Token doesn't start with 'hf_'\")\n    print(\"   2. Token doesn't have WRITE permission\")\n    print(\"   3. Extra spaces when copying token\")\n    exit(1)\n\n# ============================================================================\n# STEP 3: CREATE REPOSITORY\n# ============================================================================\n\nMODEL_NAME = \"nepal-legal-mistral-7b\"\nREPO_ID = f\"{username}/{MODEL_NAME}\"\n\nprint(f\"\\nüì¶ Creating repository: {REPO_ID}\")\ntry:\n    create_repo(\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        private=False,  # Public repo - change to True for private\n        exist_ok=True\n    )\n    print(f\"‚úÖ Repository created!\")\n    print(f\"   üîó https://huggingface.co/{REPO_ID}\")\nexcept Exception as e:\n    print(f\"‚ùå Failed: {e}\")\n    exit(1)\n\n# ============================================================================\n# STEP 4: UPLOAD MODEL FILES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì§ STARTING UPLOAD\")\nprint(\"=\"*80)\nprint(f\"\\n‚è±Ô∏è  Estimated time: 20-40 minutes\")\nprint(f\"üí° TIP: You can close this browser tab - upload continues!\")\nprint(f\"üìä Progress will be shown below...\")\nprint(\"=\"*80 + \"\\n\")\n\nMODEL_PATH = \"./nepal-legal-model-merged\"\n\n# Check if model exists\nif not os.path.exists(MODEL_PATH):\n    print(f\"‚ùå ERROR: Model not found at {MODEL_PATH}\")\n    print(\"Make sure you ran the training cells first!\")\n    exit(1)\n\napi = HfApi()\n\ntry:\n    # Upload entire folder\n    print(\"Starting upload...\")\n    api.upload_folder(\n        folder_path=MODEL_PATH,\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        repo_type=\"model\",\n        commit_message=\"Upload Nepal Legal Mistral 7B - Fine-tuned on Nepal legal data\"\n    )\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ UPLOAD COMPLETE!\")\n    print(\"=\"*80)\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Upload failed: {e}\")\n    print(\"\\nüîç Troubleshooting:\")\n    print(\"   1. Check internet connection\")\n    print(\"   2. Make sure Kaggle hasn't timed out\")\n    print(\"   3. Try running the cell again\")\n    exit(1)\n\n# ============================================================================\n# STEP 5: CREATE MODEL CARD (README)\n# ============================================================================\n\nprint(\"\\nüìù Creating model card...\")\n\nmodel_card = f\"\"\"---\nlanguage:\n- en\n- ne\nlicense: apache-2.0\ntags:\n- legal\n- nepal\n- mistral\n- fine-tuned\n- national-penal-code\n- legal-ai\nbase_model: mistralai/Mistral-7B-v0.1\nlibrary_name: transformers\npipeline_tag: text-generation\n---\n\n# Nepal Legal Mistral 7B üá≥üáµ ‚öñÔ∏è\n\nA Mistral 7B model fine-tuned specifically for Nepal legal system queries, with focus on the National Penal Code 2017.\n\n## Model Description\n\nThis model is based on **Mistral-7B-v0.1** and has been fine-tuned using LoRA (Low-Rank Adaptation) on 6,400 Nepal legal instruction-following examples. It can explain legal provisions, answer questions about Nepal law, and provide summaries of legal concepts in simple language.\n\n### Training Data\n- **Dataset Size**: 6,400 examples\n- **Sources**: \n  - National Penal Code 2017\n  - Nepal legal provisions\n  - Legal Q&A pairs\n- **Languages**: Primarily English, with Nepal legal terminology\n\n### Training Details\n- **Base Model**: mistralai/Mistral-7B-v0.1\n- **Method**: LoRA fine-tuning\n- **Trainable Parameters**: 41.9M (1.11% of total)\n- **Training Loss**: 0.4175\n- **Validation Loss**: 0.4076\n- **Training Time**: ~3 hours on Kaggle T4 GPU\n\n## Usage\n\n### Quick Start\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"{REPO_ID}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{REPO_ID}\")\n\n# Ask a legal question\nprompt = \\\"\\\"\\\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\nWhat is the punishment for theft under National Penal Code 2017?\n\n### Response:\n\\\"\\\"\\\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract just the answer\nanswer = response.split(\"### Response:\")[-1].strip()\nprint(answer)\n```\n\n### Example Questions\n\nThe model can answer questions like:\n- \"What is the punishment for theft under Nepal law?\"\n- \"Explain the scope of National Penal Code 2017\"\n- \"What are the elements of the offense of obstructing a public servant?\"\n- \"What are fundamental rights in Nepal?\"\n\n### Prompt Format\n\nFor best results, use this format:\n\n```\n### Instruction:\n[What you want the model to do - e.g., \"Explain this in simple language\"]\n\n### Input:\n[Your specific legal question]\n\n### Response:\n```\n\n## Example Outputs\n\n**Q: What is the punishment for theft under National Penal Code 2017?**\n\n> The punishment for theft under National Penal Code 2017 is imprisonment for up to seven years and a fine of up to seventy thousand rupees. This means that if someone steals something, they could end up spending time in prison and having to pay a large fine.\n\n**Q: Explain the scope and jurisdiction of National Penal Code 2017**\n\n> National Penal Code 2017 is the primary criminal law of Nepal, which sets out the criminal offenses and their corresponding punishments. The code applies to all crimes committed within the territory of Nepal, including those committed by Nepalese citizens or foreigners...\n\n## Limitations and Ethical Considerations\n\n‚ö†Ô∏è **IMPORTANT DISCLAIMERS:**\n\n1. **Not Legal Advice**: This model provides general legal information but is NOT a substitute for professional legal advice. Always consult qualified legal professionals for legal matters.\n\n2. **Training Data Limitations**: \n   - Trained primarily on National Penal Code 2017\n   - May not reflect recent legal changes\n   - Limited coverage of civil law, constitutional law, etc.\n\n3. **Language**: Primarily trained on English text with Nepal legal terminology. May not perform well on queries in Nepali language.\n\n4. **Accuracy**: While the model performs well on test cases, it can still make mistakes or hallucinate information. Always verify important legal information.\n\n5. **Bias**: May reflect biases present in the training data.\n\n## Intended Use Cases\n\n‚úÖ **Appropriate Uses:**\n- Legal education and learning\n- Quick reference for legal concepts\n- Understanding basic Nepal legal provisions\n- Research assistance\n- Explaining laws in simpler terms\n\n‚ùå **Inappropriate Uses:**\n- Making legal decisions without professional advice\n- Representing clients in court\n- Drafting legal documents without lawyer review\n- Any situation requiring certified legal expertise\n\n## Model Performance\n\n- **Training Loss**: 0.4175\n- **Validation Loss**: 0.4076\n- **Test Cases**: Successfully answered questions about:\n  - Theft and punishment\n  - National Penal Code scope\n  - Obstructing public servants\n  - And other legal provisions\n\n## Technical Specifications\n\n- **Architecture**: Mistral 7B with LoRA adapters\n- **Parameters**: 7B total (41.9M trainable)\n- **Context Length**: 256 tokens (training)\n- **Precision**: FP16\n- **Model Size**: ~13.5 GB\n\n## Citation\n\nIf you use this model in your research or application, please cite:\n\n```bibtex\n@misc{{nepal-legal-mistral-7b,\n  author = {{{username}}},\n  title = {{Nepal Legal Mistral 7B}},\n  year = {{2025}},\n  publisher = {{HuggingFace}},\n  url = {{https://huggingface.co/{REPO_ID}}}\n}}\n```\n\n## License\n\nThis model is released under the Apache 2.0 license, same as the base Mistral model.\n\n## Acknowledgments\n\n- Base model: [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) by Mistral AI\n- Training platform: Kaggle\n- Framework: Hugging Face Transformers + PEFT\n\n## Contact\n\nFor questions, issues, or feedback about this model, please open an issue on the model repository.\n\n---\n\n**Disclaimer**: This is an AI model for educational and research purposes. It does not provide legal advice and should not be used as a substitute for consultation with qualified legal professionals.\n\"\"\"\n\ntry:\n    api.upload_file(\n        path_or_fileobj=model_card.encode('utf-8'),\n        path_in_repo=\"README.md\",\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        commit_message=\"Add comprehensive model card\"\n    )\n    print(\"‚úÖ Model card uploaded\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Could not upload model card: {e}\")\n    print(\"   You can add it manually later\")\n\n# ============================================================================\n# FINAL SUCCESS MESSAGE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéâ ALL DONE! YOUR MODEL IS NOW PUBLIC!\")\nprint(\"=\"*80)\n\nprint(f\"\\nüîó View your model:\")\nprint(f\"   https://huggingface.co/{REPO_ID}\")\n\nprint(f\"\\nüíª Anyone can now use it:\")\nprint(f\"\"\"\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"{REPO_ID}\")\nresult = pipe(\"What is theft under Nepal law?\")\nprint(result[0]['generated_text'])\n\"\"\")\n\nprint(f\"\\nüì± Share your model:\")\nprint(f\"   https://huggingface.co/{REPO_ID}\")\n\nprint(\"\\n‚ú® Congratulations on creating and sharing your Nepal Legal AI! üá≥üáµ‚öñÔ∏è\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:17:41.028366Z","iopub.execute_input":"2026-01-15T17:17:41.028698Z","iopub.status.idle":"2026-01-15T17:21:12.395617Z","shell.execute_reply.started":"2026-01-15T17:17:41.028672Z","shell.execute_reply":"2026-01-15T17:21:12.394932Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüöÄ UPLOADING TO HUGGINGFACE\n================================================================================\n\nüîë Logging in to HuggingFace...\n","output_type":"stream"},{"name":"stderr","text":"Token has not been saved to git credential helper.\nWARNING:huggingface_hub._login:Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n‚úÖ Success! Logged in as: yamraj047\n\nüì¶ Creating repository: yamraj047/nepal-legal-mistral-7b\n‚úÖ Repository created!\n   üîó https://huggingface.co/yamraj047/nepal-legal-mistral-7b\n\n================================================================================\nüì§ STARTING UPLOAD\n================================================================================\n\n‚è±Ô∏è  Estimated time: 20-40 minutes\nüí° TIP: You can close this browser tab - upload continues!\nüìä Progress will be shown below...\n================================================================================\n\nStarting upload...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadb2de9ff7749ec91439e5da254fefe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eda59c513a9468591fc338049523ebc"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\n‚úÖ UPLOAD COMPLETE!\n================================================================================\n\nüìù Creating model card...\n‚úÖ Model card uploaded\n\n================================================================================\nüéâ ALL DONE! YOUR MODEL IS NOW PUBLIC!\n================================================================================\n\nüîó View your model:\n   https://huggingface.co/yamraj047/nepal-legal-mistral-7b\n\nüíª Anyone can now use it:\n\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"yamraj047/nepal-legal-mistral-7b\")\nresult = pipe(\"What is theft under Nepal law?\")\nprint(result[0]['generated_text'])\n\n\nüì± Share your model:\n   https://huggingface.co/yamraj047/nepal-legal-mistral-7b\n\n‚ú® Congratulations on creating and sharing your Nepal Legal AI! üá≥üáµ‚öñÔ∏è\n================================================================================\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# ADVANCED FEATURES FOR NEPAL LEGAL AI MODEL\n# Add these cells to your Kaggle notebook for enhanced functionality\n# ============================================================================\n\n\"\"\"\nCell 15: RELOAD MODEL FOR ADVANCED FEATURES\n(Your model was deleted after upload to save memory)\n\"\"\"\nprint(\"üîÑ Reloading model for advanced features...\")\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Reload your merged model\nMERGED_OUTPUT_DIR = \"./nepal-legal-model-merged\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MERGED_OUTPUT_DIR,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT_DIR)\n\nprint(\"‚úÖ Model reloaded and ready!\")\nprint(f\"   Model device: {model.device}\")\n\n# ============================================================================\n# FEATURE 1: RAG (Retrieval Augmented Generation) System\n# Combines vector search with your model for more accurate answers\n# ============================================================================\n\n\"\"\"\nCell 16: Install RAG dependencies\n\"\"\"\n!pip install -q faiss-cpu sentence-transformers langchain\n\n\"\"\"\nCell 2: Build Legal Document Vector Database\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nimport pickle\n\nprint(\"üîç Building RAG System for Nepal Legal Documents\")\n\n# Sample legal documents (replace with your actual legal corpus)\nlegal_documents = [\n    {\n        \"text\": \"National Penal Code 2017, Section 177: Theft is punishable by imprisonment up to 7 years and fine up to NPR 70,000\",\n        \"source\": \"NPC 2017 Section 177\",\n        \"category\": \"Property Crimes\"\n    },\n    {\n        \"text\": \"National Penal Code 2017, Section 189: Murder carries life imprisonment or imprisonment up to 20 years\",\n        \"source\": \"NPC 2017 Section 189\",\n        \"category\": \"Crimes Against Person\"\n    },\n    # Add more documents here\n]\n\n# Initialize embedding model\nprint(\"Loading embedding model...\")\nembedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# Create embeddings\nprint(\"Creating document embeddings...\")\ntexts = [doc[\"text\"] for doc in legal_documents]\nembeddings = embedder.encode(texts, show_progress_bar=True)\n\n# Build FAISS index\nprint(\"Building FAISS index...\")\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings.astype('float32'))\n\n# Save for later use\nprint(\"Saving RAG database...\")\nwith open('legal_docs.pkl', 'wb') as f:\n    pickle.dump(legal_documents, f)\nfaiss.write_index(index, 'legal_index.faiss')\n\nprint(\"‚úÖ RAG System ready!\")\n\n\"\"\"\nCell 3: RAG-Enhanced Query Function\n\"\"\"\ndef rag_enhanced_query(question, top_k=3):\n    \"\"\"Query with RAG: retrieve relevant docs + generate answer\"\"\"\n    \n    # Load resources\n    with open('legal_docs.pkl', 'rb') as f:\n        docs = pickle.load(f)\n    index = faiss.read_index('legal_index.faiss')\n    \n    # Embed query\n    query_embedding = embedder.encode([question])\n    \n    # Search similar documents\n    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n    \n    # Get relevant docs\n    context = \"\\n\\n\".join([docs[i][\"text\"] for i in indices[0]])\n    \n    # Create enhanced prompt\n    prompt = f\"\"\"### Instruction:\nAnswer this legal question using the provided context from Nepal law.\n\n### Context:\n{context}\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n    \n    # Generate answer\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = answer.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"answer\": answer,\n        \"sources\": [docs[i][\"source\"] for i in indices[0]],\n        \"context\": context\n    }\n\n# Test it\nresult = rag_enhanced_query(\"What is the punishment for theft?\")\nprint(\"Answer:\", result[\"answer\"])\nprint(\"\\nSources:\", result[\"sources\"])\n\n\n# ============================================================================\n# FEATURE 2: Multi-Language Support (English + Nepali)\n# ============================================================================\n\n\"\"\"\nCell 4: Install translation libraries\n\"\"\"\n!pip install -q googletrans==4.0.0-rc1 indic-nlp-library","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:19:59.914720Z","iopub.execute_input":"2026-01-15T18:19:59.915446Z","iopub.status.idle":"2026-01-15T18:21:06.573961Z","shell.execute_reply.started":"2026-01-15T18:19:59.915415Z","shell.execute_reply":"2026-01-15T18:21:06.573210Z"}},"outputs":[{"name":"stdout","text":"üîÑ Reloading model for advanced features...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4b6eac9d964284ace0452a99f5eacc"}},"metadata":{}},{"name":"stderr","text":"WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model reloaded and ready!\n   Model device: cuda:0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.28.1 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0müîç Building RAG System for Nepal Legal Documents\nLoading embedding model...\nCreating document embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ccabda9e8d40b9b98c5ba397f5a4de"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Building FAISS index...\nSaving RAG database...\n‚úÖ RAG System ready!\nAnswer: ording to the National Penal Code 2017, Section 177, theft is punishable by imprisonment up to 7 years and a fine up to NPR 70,000. This means that if a person is found guilty of theft, they can be sentenced to imprisonment for a maximum of 7 years and/or fined up to NPR 70,000.\n\nIt is important to note that the punishment for theft can vary depending on the circumstances of the case, such as the value of the property stolen, the method used to steal it, and the severity of the crime. Additionally, if the theft was committed with the intention of committing murder, the punishment can be more severe, as per Section 189 of the National Penal Code 2017.\n\nIn conclusion, the punishment for theft in Nepal can be severe, and it is important for individuals to understand the legal consequences of committing such crimes. If you have any further questions or concerns, it is recommended that you seek legal advice from a qualified attorney.\n\nSources: ['NPC 2017 Section 177', 'NPC 2017 Section 189', 'NPC 2017 Section 189']\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngradio-client 1.13.3 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\nopenai 1.109.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\ngradio 5.49.1 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nfirebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\ngoogle-genai 1.45.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\nlangsmith 0.4.37 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\nmcp 1.18.0 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nCell 5: Language Detection & Translation\n\"\"\"\nfrom googletrans import Translator\n\ntranslator = Translator()\n\ndef smart_query(question):\n    \"\"\"Automatically detect language and translate if needed\"\"\"\n    \n    # Detect language\n    detected = translator.detect(question)\n    \n    if detected.lang == 'ne':  # Nepali\n        print(\"üá≥üáµ Detected Nepali - Translating to English...\")\n        question_en = translator.translate(question, src='ne', dest='en').text\n        print(f\"Translated: {question_en}\")\n    else:\n        question_en = question\n    \n    # Get answer in English\n    prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question_en}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)\n    answer_en = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer_en = answer_en.split(\"### Response:\")[-1].strip()\n    \n    # Translate back if original was Nepali\n    if detected.lang == 'ne':\n        print(\"\\nüîÑ Translating answer back to Nepali...\")\n        answer = translator.translate(answer_en, src='en', dest='ne').text\n    else:\n        answer = answer_en\n    \n    return {\n        \"question\": question,\n        \"question_en\": question_en,\n        \"answer\": answer,\n        \"answer_en\": answer_en,\n        \"detected_lang\": detected.lang\n    }\n\n# Test\nresult = smart_query(\"‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\")  # \"What is punishment for theft?\"\nprint(\"Answer:\", result[\"answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:21:31.910589Z","iopub.execute_input":"2026-01-15T18:21:31.910988Z","iopub.status.idle":"2026-01-15T18:21:31.928542Z","shell.execute_reply.started":"2026-01-15T18:21:31.910953Z","shell.execute_reply":"2026-01-15T18:21:31.927532Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1521038901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCell\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLanguage\u001b[0m \u001b[0mDetection\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mTranslation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLANGCODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANGUAGES\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mRPC_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MkEWBc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \"\"\"Google Translate ajax API implementation class\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mTranslator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     def __init__(self, service_urls=DEFAULT_CLIENT_SERVICE_URLS, user_agent=DEFAULT_USER_AGENT,\n\u001b[1;32m     61\u001b[0m                  \u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_RAISE_EXCEPTION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                  \u001b[0mproxies\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttpcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyncHTTPTransport\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                  \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                  \u001b[0mhttp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'httpcore' has no attribute 'SyncHTTPTransport'"],"ename":"AttributeError","evalue":"module 'httpcore' has no attribute 'SyncHTTPTransport'","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"\"\"\"\nCell: Language Detection & Translation (FIXED VERSION)\nUses deep-translator instead of googletrans (more reliable)\n\"\"\"\n\n# Install the working translation library\n!pip install -q deep-translator\n\nfrom deep_translator import GoogleTranslator\nfrom langdetect import detect\nimport langdetect\n\nprint(\"‚úÖ Translation libraries installed successfully\")\n\ndef smart_query(question):\n    \"\"\"Automatically detect language and translate if needed\"\"\"\n    \n    try:\n        # Detect language using langdetect\n        detected_lang = detect(question)\n        print(f\"üîç Detected language: {detected_lang}\")\n        \n        if detected_lang == 'ne':  # Nepali\n            print(\"üá≥üáµ Detected Nepali - Translating to English...\")\n            \n            # Translate to English\n            translator_to_en = GoogleTranslator(source='ne', target='en')\n            question_en = translator_to_en.translate(question)\n            print(f\"Translated: {question_en}\")\n        else:\n            question_en = question\n        \n        # Get answer in English using the model\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question_en}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=300, \n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        answer_en = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer_en = answer_en.split(\"### Response:\")[-1].strip()\n        \n        # Translate back if original was Nepali\n        if detected_lang == 'ne':\n            print(\"\\nüîÑ Translating answer back to Nepali...\")\n            translator_to_ne = GoogleTranslator(source='en', target='ne')\n            answer = translator_to_ne.translate(answer_en)\n        else:\n            answer = answer_en\n        \n        return {\n            \"question\": question,\n            \"question_en\": question_en,\n            \"answer\": answer,\n            \"answer_en\": answer_en,\n            \"detected_lang\": detected_lang\n        }\n    \n    except Exception as e:\n        print(f\"‚ùå Translation error: {e}\")\n        print(\"‚ÑπÔ∏è  Falling back to English-only mode...\")\n        \n        # Fallback: just process in English\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=300, \n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        return {\n            \"question\": question,\n            \"question_en\": question,\n            \"answer\": answer,\n            \"answer_en\": answer,\n            \"detected_lang\": \"unknown\"\n        }\n\n\n# ============================================================================\n# TEST CASES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING TRANSLATION SYSTEM\")\nprint(\"=\"*80)\n\n# Test 1: Nepali question\nprint(\"\\nüìù Test 1: Nepali Question\")\nprint(\"-\" * 80)\ntry:\n    result1 = smart_query(\"‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\")  # \"What is punishment for theft?\"\n    print(f\"\\nüá≥üáµ Original: {result1['question']}\")\n    print(f\"üá¨üáß English: {result1['question_en']}\")\n    print(f\"\\nüí¨ Answer (Nepali): {result1['answer']}\")\n    print(f\"\\nüí¨ Answer (English): {result1['answer_en']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 1 failed: {e}\")\n\n# Test 2: English question\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìù Test 2: English Question\")\nprint(\"-\" * 80)\ntry:\n    result2 = smart_query(\"What is the punishment for theft under Nepal law?\")\n    print(f\"\\nüá¨üáß Question: {result2['question']}\")\n    print(f\"\\nüí¨ Answer: {result2['answer']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 2 failed: {e}\")\n\n# Test 3: Another Nepali question\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìù Test 3: Another Nepali Question\")\nprint(\"-\" * 80)\ntry:\n    result3 = smart_query(\"‡§π‡§§‡•ç‡§Ø‡§æ‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\")  # \"What is punishment for murder?\"\n    print(f\"\\nüá≥üáµ Original: {result3['question']}\")\n    print(f\"üá¨üáß English: {result3['question_en']}\")\n    print(f\"\\nüí¨ Answer (Nepali): {result3['answer']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 3 failed: {e}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ Translation system testing complete!\")\nprint(\"=\"*80)\n\n\n# ============================================================================\n# ALTERNATIVE: Manual Translation Helper (if auto-detection fails)\n# ============================================================================\n\ndef translate_nepali_to_english(nepali_text):\n    \"\"\"Simple one-way Nepali to English translation\"\"\"\n    try:\n        translator = GoogleTranslator(source='ne', target='en')\n        return translator.translate(nepali_text)\n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return nepali_text\n\ndef translate_english_to_nepali(english_text):\n    \"\"\"Simple one-way English to Nepali translation\"\"\"\n    try:\n        translator = GoogleTranslator(source='en', target='ne')\n        return translator.translate(english_text)\n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return english_text\n\n# Manual translation examples\nprint(\"\\n\" + \"=\"*80)\nprint(\"MANUAL TRANSLATION HELPERS\")\nprint(\"=\"*80)\n\nprint(\"\\nüá≥üáµ ‚Üí üá¨üáß Nepali to English:\")\nprint(translate_nepali_to_english(\"‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\"))\n\nprint(\"\\nüá¨üáß ‚Üí üá≥üáµ English to Nepali:\")\nprint(translate_english_to_nepali(\"What is the punishment for theft?\"))\n\nprint(\"\\n‚úÖ All functions ready to use!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:28:44.170226Z","iopub.execute_input":"2026-01-15T18:28:44.170575Z","iopub.status.idle":"2026-01-15T18:28:48.270534Z","shell.execute_reply.started":"2026-01-15T18:28:44.170544Z","shell.execute_reply":"2026-01-15T18:28:48.269523Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m553.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3189869791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeep_translator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangdetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'"],"ename":"ModuleNotFoundError","evalue":"No module named 'langdetect'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"\"\"\"\nCell: Language Detection & Translation (FIXED VERSION)\nUses deep-translator instead of googletrans (more reliable)\n\"\"\"\n\n# Install the working translation libraries\n!pip install -q deep-translator langdetect\n\nfrom deep_translator import GoogleTranslator\nfrom langdetect import detect\nimport langdetect\n\nprint(\"‚úÖ Translation libraries installed successfully\")\n\ndef smart_query(question):\n    \"\"\"Automatically detect language and translate if needed\"\"\"\n    \n    try:\n        # Detect language using langdetect\n        detected_lang = detect(question)\n        print(f\"üîç Detected language: {detected_lang}\")\n        \n        if detected_lang == 'ne':  # Nepali\n            print(\"üá≥üáµ Detected Nepali - Translating to English...\")\n            \n            # Translate to English\n            translator_to_en = GoogleTranslator(source='ne', target='en')\n            question_en = translator_to_en.translate(question)\n            print(f\"Translated: {question_en}\")\n        else:\n            question_en = question\n        \n        # Get answer in English using the model\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question_en}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=300, \n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        answer_en = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer_en = answer_en.split(\"### Response:\")[-1].strip()\n        \n        # Translate back if original was Nepali\n        if detected_lang == 'ne':\n            print(\"\\nüîÑ Translating answer back to Nepali...\")\n            translator_to_ne = GoogleTranslator(source='en', target='ne')\n            answer = translator_to_ne.translate(answer_en)\n        else:\n            answer = answer_en\n        \n        return {\n            \"question\": question,\n            \"question_en\": question_en,\n            \"answer\": answer,\n            \"answer_en\": answer_en,\n            \"detected_lang\": detected_lang\n        }\n    \n    except Exception as e:\n        print(f\"‚ùå Translation error: {e}\")\n        print(\"‚ÑπÔ∏è  Falling back to English-only mode...\")\n        \n        # Fallback: just process in English\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=300, \n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        return {\n            \"question\": question,\n            \"question_en\": question,\n            \"answer\": answer,\n            \"answer_en\": answer,\n            \"detected_lang\": \"unknown\"\n        }\n\n\n# ============================================================================\n# TEST CASES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING TRANSLATION SYSTEM\")\nprint(\"=\"*80)\n\n# Test 1: Nepali question\nprint(\"\\nüìù Test 1: Nepali Question\")\nprint(\"-\" * 80)\ntry:\n    result1 = smart_query(\"‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\")  # \"What is punishment for theft?\"\n    print(f\"\\nüá≥üáµ Original: {result1['question']}\")\n    print(f\"üá¨üáß English: {result1['question_en']}\")\n    print(f\"\\nüí¨ Answer (Nepali): {result1['answer']}\")\n    print(f\"\\nüí¨ Answer (English): {result1['answer_en']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 1 failed: {e}\")\n\n# Test 2: English question\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìù Test 2: English Question\")\nprint(\"-\" * 80)\ntry:\n    result2 = smart_query(\"What is the punishment for theft under Nepal law?\")\n    print(f\"\\nüá¨üáß Question: {result2['question']}\")\n    print(f\"\\nüí¨ Answer: {result2['answer']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 2 failed: {e}\")\n\n# Test 3: Another Nepali question\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìù Test 3: Another Nepali Question\")\nprint(\"-\" * 80)\ntry:\n    result3 = smart_query(\"‡§π‡§§‡•ç‡§Ø‡§æ‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\")  # \"What is punishment for murder?\"\n    print(f\"\\nüá≥üáµ Original: {result3['question']}\")\n    print(f\"üá¨üáß English: {result3['question_en']}\")\n    print(f\"\\nüí¨ Answer (Nepali): {result3['answer']}\")\nexcept Exception as e:\n    print(f\"‚ùå Test 3 failed: {e}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ Translation system testing complete!\")\nprint(\"=\"*80)\n\n\n# ============================================================================\n# ALTERNATIVE: Manual Translation Helper (if auto-detection fails)\n# ============================================================================\n\ndef translate_nepali_to_english(nepali_text):\n    \"\"\"Simple one-way Nepali to English translation\"\"\"\n    try:\n        translator = GoogleTranslator(source='ne', target='en')\n        return translator.translate(nepali_text)\n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return nepali_text\n\ndef translate_english_to_nepali(english_text):\n    \"\"\"Simple one-way English to Nepali translation\"\"\"\n    try:\n        translator = GoogleTranslator(source='en', target='ne')\n        return translator.translate(english_text)\n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return english_text\n\n# Manual translation examples\nprint(\"\\n\" + \"=\"*80)\nprint(\"MANUAL TRANSLATION HELPERS\")\nprint(\"=\"*80)\n\nprint(\"\\nüá≥üáµ ‚Üí üá¨üáß Nepali to English:\")\nprint(translate_nepali_to_english(\"‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\"))\n\nprint(\"\\nüá¨üáß ‚Üí üá≥üáµ English to Nepali:\")\nprint(translate_english_to_nepali(\"What is the punishment for theft?\"))\n\nprint(\"\\n‚úÖ All functions ready to use!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:29:40.740222Z","iopub.execute_input":"2026-01-15T18:29:40.740578Z","iopub.status.idle":"2026-01-15T18:32:32.748295Z","shell.execute_reply.started":"2026-01-15T18:29:40.740544Z","shell.execute_reply":"2026-01-15T18:32:32.747558Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n‚úÖ Translation libraries installed successfully\n\n================================================================================\nTESTING TRANSLATION SYSTEM\n================================================================================\n\nüìù Test 1: Nepali Question\n--------------------------------------------------------------------------------\nüîç Detected language: hi\n\nüá≥üáµ Original: ‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\nüá¨üáß English: ‡§ö‡•ã‡§∞‡•Ä‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\n\nüí¨ Answer (Nepali): legal provision in question is a part of Nepal's National Penal Code 2017, which outlines the punishment for theft. Specifically, it states that if someone is convicted of theft and is sentenced to imprisonment, but they have already paid back the amount they stole to the victim or to someone else, then the court can reduce their sentence by up to 50%.\n\nIn simpler terms, this means that if someone steals something and is caught and sentenced to prison, but they return the stolen item to the victim or someone else, the court may reduce their sentence by half. This is intended to encourage people to take responsibility for their actions and make things right with the person they stole from.\n\nIt's important to note that this provision only applies if the person was convicted of theft and was sentenced to imprisonment. It does not apply to other types of crimes or to other forms of punishment. Additionally, the amount of the reduction in sentence will depend on the severity of the theft and other factors involved in the case.\n\nOverall, this provision is intended to promote justice and accountability by allowing people to take responsibility for their actions and make things right with the person they stole from. By reducing the sentence for those who do this, the court is incentivizing people to behave ethically and respect the rights of others.\n\nüí¨ Answer (English): legal provision in question is a part of Nepal's National Penal Code 2017, which outlines the punishment for theft. Specifically, it states that if someone is convicted of theft and is sentenced to imprisonment, but they have already paid back the amount they stole to the victim or to someone else, then the court can reduce their sentence by up to 50%.\n\nIn simpler terms, this means that if someone steals something and is caught and sentenced to prison, but they return the stolen item to the victim or someone else, the court may reduce their sentence by half. This is intended to encourage people to take responsibility for their actions and make things right with the person they stole from.\n\nIt's important to note that this provision only applies if the person was convicted of theft and was sentenced to imprisonment. It does not apply to other types of crimes or to other forms of punishment. Additionally, the amount of the reduction in sentence will depend on the severity of the theft and other factors involved in the case.\n\nOverall, this provision is intended to promote justice and accountability by allowing people to take responsibility for their actions and make things right with the person they stole from. By reducing the sentence for those who do this, the court is incentivizing people to behave ethically and respect the rights of others.\n\n================================================================================\n\nüìù Test 2: English Question\n--------------------------------------------------------------------------------\nüîç Detected language: en\n\nüá¨üáß Question: What is the punishment for theft under Nepal law?\n\nüí¨ Answer: legal provision states that if someone steals a government or a public building, or a part of it, they will be punished with imprisonment for up to five years, or a fine, or both. Similarly, if someone steals property that is worth more than 50,000 rupees, but less than 200,000 rupees, they will also be punished with imprisonment for up to five years, or a fine, or both. If someone steals property that is worth less than 50,000 rupees, they will be punished with imprisonment for up to three years, or a fine, or both. If someone steals property that is worth more than 200,000 rupees, they will be punished with imprisonment for up to seven years, or a fine, or both. Finally, if someone steals property that is worth less than 200,000 rupees, but more than 50,000 rupees, they will be punished with imprisonment for up to six years, or a fine, or both. In summary, the punishment for theft in Nepal can vary depending on the value of the property stolen and the type of property stolen, and can include imprisonment and fines. It is important to note that these are the maximum sentences that can be imposed for theft off\n\n================================================================================\n\nüìù Test 3: Another Nepali Question\n--------------------------------------------------------------------------------\nüîç Detected language: hi\n\nüá≥üáµ Original: ‡§π‡§§‡•ç‡§Ø‡§æ‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\nüá¨üáß English: ‡§π‡§§‡•ç‡§Ø‡§æ‡§ï‡•ã ‡§∏‡§ú‡§æ‡§Ø ‡§ï‡•á ‡§π‡•ã?\n\nüí¨ Answer (Nepali): legal provision states that if a person is found guilty of causing death or injury to another person, the punishment for theft or robbery will not be imposed on them. This means that if someone is convicted of murder or manslaughter, they will not be sentenced to time in prison for theft or robbery. This provision is in place to ensure that the punishment for a serious crime like murder or manslaughter is not reduced or lessened in any way. It is important to note that this provision only applies to the punishment for theft or robbery and does not affect any other penalties that may be imposed on the convicted person. Overall, this legal provision is intended to ensure that the justice system is fair and that the punishment for a serious crime like murder or manslaughter is not diminished in any way.\n\n================================================================================\n‚úÖ Translation system testing complete!\n================================================================================\n\n================================================================================\nMANUAL TRANSLATION HELPERS\n================================================================================\n\nüá≥üáµ ‚Üí üá¨üáß Nepali to English:\nWhat is the punishment for theft?\n\nüá¨üáß ‚Üí üá≥üáµ English to Nepali:\n‡§ö‡•ã‡§∞‡•Ä ‡§ó‡§∞‡•ç‡§®‡•á‡§≤‡§æ‡§à ‡§ï‡•á ‡§∏‡§ú‡§æ‡§Ø ‡§π‡•Å‡§®‡•ç‡§õ ?\n\n‚úÖ All functions ready to use!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# FEATURE 3: Legal Case Analyzer\n# Analyze case scenarios and provide legal insights\n# ============================================================================\n\n\"\"\"\nCell 6: Case Analysis System\n\"\"\"\ndef analyze_legal_case(case_description):\n    \"\"\"Analyze a legal case and identify applicable laws\"\"\"\n    \n    # Step 1: Identify potential offenses\n    offense_prompt = f\"\"\"### Instruction:\nIdentify all potential criminal offenses in this case under Nepal law.\n\n### Input:\nCase: {case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(offense_prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5)\n    offenses = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    offenses = offenses.split(\"### Response:\")[-1].strip()\n    \n    # Step 2: Get applicable laws\n    law_prompt = f\"\"\"### Instruction:\nWhat sections of National Penal Code 2017 apply to this case?\n\n### Input:\n{case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(law_prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5)\n    laws = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    laws = laws.split(\"### Response:\")[-1].strip()\n    \n    # Step 3: Suggest possible punishments\n    punishment_prompt = f\"\"\"### Instruction:\nWhat are the possible punishments for this case under Nepal law?\n\n### Input:\n{case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(punishment_prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5)\n    punishment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    punishment = punishment.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"case\": case_description,\n        \"identified_offenses\": offenses,\n        \"applicable_laws\": laws,\n        \"possible_punishment\": punishment\n    }\n\n# Test\ncase = \"A person broke into a house at night and stole jewelry worth NPR 500,000\"\nanalysis = analyze_legal_case(case)\n\nprint(\"=\" * 80)\nprint(\"LEGAL CASE ANALYSIS\")\nprint(\"=\" * 80)\nprint(f\"\\nCase: {analysis['case']}\")\nprint(f\"\\nOffenses: {analysis['identified_offenses']}\")\nprint(f\"\\nApplicable Laws: {analysis['applicable_laws']}\")\nprint(f\"\\nPunishment: {analysis['possible_punishment']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:34:55.509992Z","iopub.execute_input":"2026-01-15T18:34:55.510370Z","iopub.status.idle":"2026-01-15T18:37:06.291335Z","shell.execute_reply.started":"2026-01-15T18:34:55.510338Z","shell.execute_reply":"2026-01-15T18:37:06.290565Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"================================================================================\nLEGAL CASE ANALYSIS\n================================================================================\n\nCase: A person broke into a house at night and stole jewelry worth NPR 500,000\n\nOffenses: ed on the given case, the potential criminal offenses in this case under Nepal law are:\n\n1. Burglary: According to Section 211 of the National Penal Code 2017, burglary is defined as the act of breaking into a house at night with the intention of committing a crime. In this case, the person broke into a house at night with the intention of stealing jewelry, which is a crime under Section 211.\n\n2. Theft: According to Section 212 of the National Penal Code 2017, theft is defined as the act of taking something of value without the owner's consent. In this case, the person stole jewelry worth NPR 500,000, which is a crime under Section 212.\n\n3. Criminal Trespass: According to Section 213 of the National Penal Code\n\nApplicable Laws: ional Penal Code 2017 applies to this case under the provisions of Chapter 11, Offences against Property. Specifically, Section 111 of the Code deals with the offence of burglary, which is defined as the act of breaking into a house at night with the intention of committing theft.\n\nUnder Section 111, the punishment for burglary is imprisonment for a term of three to seven years, or a fine, or both. Additionally, the court may also order the confiscation of any property or goods that were stolen during the burglary.\n\nIn this case, the person broke into a house at night and stole jewelry worth NPR 500,000. This act would constitute the offence of burglary under Section 111 of the National Penal Code 2017. The punishment for this offence would be imprisonment for a term\n\nPunishment: ed on the legal provision provided, the possible punishments for this case under Nepal law are as follows:\n\n1. Imprisonment for a term of not less than three years and not more than ten years, or a fine, or both.\n2. If the theft was committed with the use of a deadly weapon, the punishment may be imprisonment for a term of not less than five years and not more than fifteen years, or a fine, or both.\n3. If the theft was committed by a person who has previously been convicted of a theft offense, the punishment may be imprisonment for a term of not less than five years and not more than fifteen years, or a fine, or both.\n\nIt is important to note that the punishment may vary depending on the specific circumstances of the case, such as the value of the stolen property, the use of a deadly weapon, and the prior criminal record of the accused. Additionally, the punishment may be\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# FEATURE 4: Interactive Legal Chatbot with Memory\n# ============================================================================\n\n\"\"\"\nCell 7: Chatbot with Conversation History\n\"\"\"\nclass LegalChatbot:\n    def __init__(self, model, tokenizer, max_history=5):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.history = []\n        self.max_history = max_history\n    \n    def chat(self, user_input):\n        \"\"\"Chat with memory of previous exchanges\"\"\"\n        \n        # Build context from history\n        context = \"\\n\".join([\n            f\"User: {h['user']}\\nAssistant: {h['assistant']}\"\n            for h in self.history[-self.max_history:]\n        ])\n        \n        # Create prompt with context\n        if context:\n            prompt = f\"\"\"### Instruction:\nAnswer this legal question. Use the conversation history for context.\n\n### Previous Conversation:\n{context}\n\n### Input:\n{user_input}\n\n### Response:\n\"\"\"\n        else:\n            prompt = f\"\"\"### Instruction:\nAnswer this legal question about Nepal law.\n\n### Input:\n{user_input}\n\n### Response:\n\"\"\"\n        \n        # Generate response\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.7,\n            top_p=0.9\n        )\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.split(\"### Response:\")[-1].strip()\n        \n        # Update history\n        self.history.append({\n            \"user\": user_input,\n            \"assistant\": response\n        })\n        \n        return response\n    \n    def reset(self):\n        \"\"\"Clear conversation history\"\"\"\n        self.history = []\n        print(\"‚úÖ Conversation history cleared\")\n\n# Create chatbot\nchatbot = LegalChatbot(test_model, test_tokenizer)\n\n# Example conversation\nprint(\"User: What is theft?\")\nprint(\"Bot:\", chatbot.chat(\"What is theft?\"))\n\nprint(\"\\nUser: What is the punishment for it?\")  # \"it\" refers to theft from context\nprint(\"Bot:\", chatbot.chat(\"What is the punishment for it?\"))\n\nprint(\"\\nUser: Are there different types?\")\nprint(\"Bot:\", chatbot.chat(\"Are there different types?\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:38:05.790586Z","iopub.execute_input":"2026-01-15T18:38:05.790929Z","iopub.status.idle":"2026-01-15T18:38:05.802640Z","shell.execute_reply.started":"2026-01-15T18:38:05.790901Z","shell.execute_reply":"2026-01-15T18:38:05.801672Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/376124060.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Create chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mchatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLegalChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Example conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"],"ename":"NameError","evalue":"name 'test_model' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# FEATURE 5: Batch Legal Document Processor\n# Process multiple legal queries at once\n# ============================================================================\n\n\"\"\"\nCell 8: Batch Processing\n\"\"\"\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef batch_legal_analysis(questions_list, output_file=\"legal_analysis.csv\"):\n    \"\"\"Process multiple legal questions in batch\"\"\"\n    \n    results = []\n    \n    for question in tqdm(questions_list, desc=\"Processing queries\"):\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        results.append({\n            \"question\": question,\n            \"answer\": answer\n        })\n    \n    # Save to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(output_file, index=False, encoding='utf-8')\n    \n    print(f\"‚úÖ Processed {len(results)} questions\")\n    print(f\"üìÑ Results saved to {output_file}\")\n    \n    return df\n\n# Test batch processing\nquestions = [\n    \"What is murder under Nepal law?\",\n    \"What is the punishment for assault?\",\n    \"Explain the right to property\",\n    \"What is defamation?\",\n    \"What are the elements of fraud?\"\n]\n\nresults_df = batch_legal_analysis(questions)\nprint(results_df)\n\n\n# ============================================================================\n# FEATURE 6: Legal Document Summarizer\n# ============================================================================\n\n\"\"\"\nCell 9: Document Summarization\n\"\"\"\ndef summarize_legal_document(long_text, summary_type=\"brief\"):\n    \"\"\"Summarize long legal documents\"\"\"\n    \n    if summary_type == \"brief\":\n        instruction = \"Provide a brief 2-3 sentence summary of this legal text.\"\n        max_tokens = 150\n    elif summary_type == \"detailed\":\n        instruction = \"Provide a detailed summary of this legal text, including key points.\"\n        max_tokens = 400\n    else:\n        instruction = \"Extract the main points from this legal text.\"\n        max_tokens = 300\n    \n    prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{long_text}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.5)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    summary = summary.split(\"### Response:\")[-1].strip()\n    \n    return summary\n\n# Test\nlong_legal_text = \"\"\"\nNational Penal Code 2017 Section 177 deals with theft. According to this section,\ntheft is defined as the dishonest taking of movable property belonging to another\nperson without their consent. The punishment for theft includes imprisonment for\na term which may extend to seven years and also fine which may extend to seventy\nthousand rupees. The severity of punishment may vary based on the value of stolen\nproperty and circumstances of the theft.\n\"\"\"\n\nprint(\"Brief Summary:\")\nprint(summarize_legal_document(long_legal_text, \"brief\"))\n\nprint(\"\\nDetailed Summary:\")\nprint(summarize_legal_document(long_legal_text, \"detailed\"))\n\n\n# ============================================================================\n# FEATURE 7: Model Comparison & A/B Testing\n# ============================================================================\n\n\"\"\"\nCell 10: Compare Different Model Configurations\n\"\"\"\ndef compare_temperatures(question, temperatures=[0.3, 0.5, 0.7, 0.9]):\n    \"\"\"Test how temperature affects responses\"\"\"\n    \n    results = []\n    \n    prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    for temp in temperatures:\n        outputs = model.generate(**inputs, max_new_tokens=200, temperature=temp)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        results.append({\n            \"temperature\": temp,\n            \"answer\": answer,\n            \"length\": len(answer)\n        })\n    \n    return pd.DataFrame(results)\n\n# Test\ncomparison = compare_temperatures(\"What is theft under Nepal law?\")\nprint(comparison)\n\n\n# ============================================================================\n# FEATURE 8: Legal Knowledge Graph Builder\n# ============================================================================\n\n\"\"\"\nCell 11: Extract Legal Relationships\n\"\"\"\nimport json\n\ndef extract_legal_entities(text):\n    \"\"\"Extract legal entities and relationships\"\"\"\n    \n    prompt = f\"\"\"### Instruction:\nExtract all legal entities (laws, crimes, punishments, persons) from this text in JSON format.\n\n### Input:\n{text}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.5)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    response = response.split(\"### Response:\")[-1].strip()\n    \n    # Try to parse JSON\n    try:\n        entities = json.loads(response)\n    except:\n        entities = {\"raw_response\": response}\n    \n    return entities\n\n# Test\ntext = \"John was charged with theft under Section 177 of National Penal Code 2017\"\nentities = extract_legal_entities(text)\nprint(json.dumps(entities, indent=2))\n\n\n# ============================================================================\n# FEATURE 9: Confidence Scoring\n# ============================================================================\n\n\"\"\"\nCell 12: Add Confidence Scores to Answers\n\"\"\"\nimport torch.nn.functional as F\n\ndef answer_with_confidence(question):\n    \"\"\"Generate answer with confidence score\"\"\"\n    \n    prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.7,\n            output_scores=True,\n            return_dict_in_generate=True\n        )\n    \n    # Calculate average confidence\n    scores = outputs.scores\n    confidences = [F.softmax(score, dim=-1).max().item() for score in scores]\n    avg_confidence = sum(confidences) / len(confidences)\n    \n    answer = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    answer = answer.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"answer\": answer,\n        \"confidence\": avg_confidence,\n        \"confidence_label\": \"High\" if avg_confidence > 0.9 else \"Medium\" if avg_confidence > 0.7 else \"Low\"\n    }\n\n# Test\nresult = answer_with_confidence(\"What is the punishment for theft?\")\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Confidence: {result['confidence']:.2%} ({result['confidence_label']})\")\n\n\n# ============================================================================\n# FEATURE 10: API Server for Model Deployment\n# ============================================================================\n\n\"\"\"\nCell 13: Create FastAPI Server\n\"\"\"\n!pip install -q fastapi uvicorn pydantic\n\n\"\"\"\nCell 14: API Implementation\n\"\"\"\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport uvicorn\n\napp = FastAPI(title=\"Nepal Legal AI API\", version=\"1.0.0\")\n\nclass LegalQuery(BaseModel):\n    question: str\n    language: str = \"en\"\n    max_length: int = 300\n\nclass LegalResponse(BaseModel):\n    question: str\n    answer: str\n    confidence: float = None\n\n@app.post(\"/query\", response_model=LegalResponse)\nasync def legal_query(query: LegalQuery):\n    \"\"\"Main endpoint for legal queries\"\"\"\n    \n    try:\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{query.question}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        outputs = model.generate(**inputs, max_new_tokens=query.max_length, temperature=0.7)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        return LegalResponse(\n            question=query.question,\n            answer=answer\n        )\n    \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"model\": \"Nepal Legal Mistral 7B\"}\n\n# Run server (in Kaggle, use this)\n# nest_asyncio is needed for Jupyter notebooks\n!pip install -q nest_asyncio\n\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Start server in background\n# uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nprint(\"‚úÖ API server code ready!\")\nprint(\"To run: uvicorn main:app --host 0.0.0.0 --port 8000\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:19:34.473747Z","iopub.execute_input":"2026-01-15T18:19:34.474446Z","iopub.status.idle":"2026-01-15T18:19:34.535220Z","shell.execute_reply.started":"2026-01-15T18:19:34.474411Z","shell.execute_reply":"2026-01-15T18:19:34.534317Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/810879742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCell\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLanguage\u001b[0m \u001b[0mDetection\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mTranslation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLANGCODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANGUAGES\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mRPC_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MkEWBc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \"\"\"Google Translate ajax API implementation class\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mTranslator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     def __init__(self, service_urls=DEFAULT_CLIENT_SERVICE_URLS, user_agent=DEFAULT_USER_AGENT,\n\u001b[1;32m     61\u001b[0m                  \u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_RAISE_EXCEPTION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                  \u001b[0mproxies\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttpcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyncHTTPTransport\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                  \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                  \u001b[0mhttp2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'httpcore' has no attribute 'SyncHTTPTransport'"],"ename":"AttributeError","evalue":"module 'httpcore' has no attribute 'SyncHTTPTransport'","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# ADVANCED FEATURES FOR NEPAL LEGAL AI MODEL\n# Each feature is independent - run only the ones you need!\n# ============================================================================\n\n\"\"\"\nCell 15: RELOAD MODEL (Required for all features)\n\"\"\"\nprint(\"üîÑ Reloading model for advanced features...\")\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nMERGED_OUTPUT_DIR = \"./nepal-legal-model-merged\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MERGED_OUTPUT_DIR,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MERGED_OUTPUT_DIR)\n\nprint(\"‚úÖ Model reloaded and ready!\")\nprint(f\"   Model device: {model.device}\")\n\n\n# ============================================================================\n# FEATURE 1: RAG (Retrieval Augmented Generation) System ‚úÖ WORKING\n# ============================================================================\n\n\"\"\"\nCell 16: Install RAG dependencies\n\"\"\"\n!pip install -q faiss-cpu sentence-transformers\n\n\"\"\"\nCell 17: Build RAG Vector Database\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nimport pickle\n\nprint(\"üîç Building RAG System for Nepal Legal Documents\")\n\n# Legal documents database - ADD YOUR OWN DOCUMENTS HERE\nlegal_documents = [\n    {\n        \"text\": \"National Penal Code 2017, Section 177: Theft is punishable by imprisonment up to 7 years and fine up to NPR 70,000\",\n        \"source\": \"NPC 2017 Section 177\",\n        \"category\": \"Property Crimes\"\n    },\n    {\n        \"text\": \"National Penal Code 2017, Section 189: Murder carries life imprisonment or imprisonment up to 20 years\",\n        \"source\": \"NPC 2017 Section 189\",\n        \"category\": \"Crimes Against Person\"\n    },\n    {\n        \"text\": \"National Penal Code 2017, Section 151: Assault causing simple hurt is punishable by imprisonment up to 1 year or fine up to NPR 10,000\",\n        \"source\": \"NPC 2017 Section 151\",\n        \"category\": \"Crimes Against Person\"\n    },\n    {\n        \"text\": \"National Penal Code 2017, Section 200: Fraud is punishable by imprisonment up to 5 years and fine up to NPR 50,000\",\n        \"source\": \"NPC 2017 Section 200\",\n        \"category\": \"Fraud\"\n    },\n    {\n        \"text\": \"Constitution of Nepal 2015, Article 16: Right to live with dignity - every person shall have the right to live with dignity\",\n        \"source\": \"Constitution 2015 Article 16\",\n        \"category\": \"Fundamental Rights\"\n    },\n]\n\n# Initialize embedding model\nprint(\"Loading embedding model...\")\nembedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# Create embeddings\nprint(\"Creating document embeddings...\")\ntexts = [doc[\"text\"] for doc in legal_documents]\nembeddings = embedder.encode(texts, show_progress_bar=True)\n\n# Build FAISS index\nprint(\"Building FAISS index...\")\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings.astype('float32'))\n\n# Save for later use\nprint(\"Saving RAG database...\")\nwith open('legal_docs.pkl', 'wb') as f:\n    pickle.dump(legal_documents, f)\nfaiss.write_index(index, 'legal_index.faiss')\n\nprint(\"‚úÖ RAG System ready!\")\nprint(f\"   Documents indexed: {len(legal_documents)}\")\n\n\"\"\"\nCell 18: RAG-Enhanced Query Function\n\"\"\"\ndef rag_query(question, top_k=3):\n    \"\"\"Query with RAG: retrieve relevant docs + generate answer\"\"\"\n    \n    # Load resources\n    with open('legal_docs.pkl', 'rb') as f:\n        docs = pickle.load(f)\n    index = faiss.read_index('legal_index.faiss')\n    \n    # Embed query\n    query_embedding = embedder.encode([question])\n    \n    # Search similar documents\n    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n    \n    # Get relevant docs\n    context = \"\\n\\n\".join([docs[i][\"text\"] for i in indices[0]])\n    \n    # Create enhanced prompt\n    prompt = f\"\"\"### Instruction:\nAnswer this legal question using the provided context from Nepal law.\n\n### Context:\n{context}\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n    \n    # Generate answer\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, do_sample=True)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    answer = answer.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"answer\": answer,\n        \"sources\": [docs[i][\"source\"] for i in indices[0]],\n        \"context\": context\n    }\n\n# Test RAG\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing RAG System\")\nprint(\"=\"*80)\nresult = rag_query(\"What is the punishment for theft?\")\nprint(f\"\\nQuestion: What is the punishment for theft?\")\nprint(f\"\\nAnswer: {result['answer']}\")\nprint(f\"\\nSources: {result['sources']}\")\n\n\n# ============================================================================\n# FEATURE 2: Legal Case Analyzer\n# ============================================================================\n\n\"\"\"\nCell 19: Case Analysis System\n\"\"\"\ndef analyze_case(case_description):\n    \"\"\"Analyze a legal case and identify applicable laws\"\"\"\n    \n    # Step 1: Identify potential offenses\n    offense_prompt = f\"\"\"### Instruction:\nIdentify all potential criminal offenses in this case under Nepal law.\n\n### Input:\nCase: {case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(offense_prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5, do_sample=True)\n    offenses = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    offenses = offenses.split(\"### Response:\")[-1].strip()\n    \n    # Step 2: Get applicable laws\n    law_prompt = f\"\"\"### Instruction:\nWhat sections of National Penal Code 2017 apply to this case?\n\n### Input:\n{case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(law_prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5, do_sample=True)\n    laws = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    laws = laws.split(\"### Response:\")[-1].strip()\n    \n    # Step 3: Suggest possible punishments\n    punishment_prompt = f\"\"\"### Instruction:\nWhat are the possible punishments for this case under Nepal law?\n\n### Input:\n{case_description}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(punishment_prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.5, do_sample=True)\n    punishment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    punishment = punishment.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"case\": case_description,\n        \"offenses\": offenses,\n        \"laws\": laws,\n        \"punishment\": punishment\n    }\n\n# Test Case Analyzer\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing Case Analyzer\")\nprint(\"=\"*80)\ncase = \"A person broke into a house at night and stole jewelry worth NPR 500,000\"\nanalysis = analyze_case(case)\n\nprint(f\"\\nüìã Case: {analysis['case']}\")\nprint(f\"\\n‚öñÔ∏è Offenses: {analysis['offenses']}\")\nprint(f\"\\nüìú Laws: {analysis['laws']}\")\nprint(f\"\\n‚ö†Ô∏è Punishment: {analysis['punishment']}\")\n\n\n# ============================================================================\n# FEATURE 3: Interactive Chatbot with Memory\n# ============================================================================\n\n\"\"\"\nCell 20: Chatbot with Conversation History\n\"\"\"\nclass LegalChatbot:\n    def __init__(self, model, tokenizer, max_history=5):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.history = []\n        self.max_history = max_history\n    \n    def chat(self, user_input):\n        \"\"\"Chat with memory of previous exchanges\"\"\"\n        \n        # Build context from history\n        context = \"\\n\".join([\n            f\"User: {h['user']}\\nAssistant: {h['assistant']}\"\n            for h in self.history[-self.max_history:]\n        ])\n        \n        # Create prompt with context\n        if context:\n            prompt = f\"\"\"### Instruction:\nAnswer this legal question. Use the conversation history for context.\n\n### Previous Conversation:\n{context}\n\n### Input:\n{user_input}\n\n### Response:\n\"\"\"\n        else:\n            prompt = f\"\"\"### Instruction:\nAnswer this legal question about Nepal law.\n\n### Input:\n{user_input}\n\n### Response:\n\"\"\"\n        \n        # Generate response\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        outputs = self.model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9\n        )\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.split(\"### Response:\")[-1].strip()\n        \n        # Update history\n        self.history.append({\n            \"user\": user_input,\n            \"assistant\": response\n        })\n        \n        return response\n    \n    def reset(self):\n        \"\"\"Clear conversation history\"\"\"\n        self.history = []\n        print(\"‚úÖ Conversation history cleared\")\n    \n    def show_history(self):\n        \"\"\"Display conversation history\"\"\"\n        if not self.history:\n            print(\"No conversation history\")\n            return\n        \n        for i, entry in enumerate(self.history, 1):\n            print(f\"\\n{i}. User: {entry['user']}\")\n            print(f\"   Bot: {entry['assistant'][:100]}...\")\n\n# Create chatbot\nchatbot = LegalChatbot(model, tokenizer)\n\n# Test chatbot\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing Chatbot with Memory\")\nprint(\"=\"*80)\n\nprint(\"\\nüë§ User: What is theft?\")\nresponse1 = chatbot.chat(\"What is theft?\")\nprint(f\"ü§ñ Bot: {response1}\")\n\nprint(\"\\nüë§ User: What is the punishment for it?\")\nresponse2 = chatbot.chat(\"What is the punishment for it?\")\nprint(f\"ü§ñ Bot: {response2}\")\n\nprint(\"\\nüë§ User: Are there different types?\")\nresponse3 = chatbot.chat(\"Are there different types?\")\nprint(f\"ü§ñ Bot: {response3}\")\n\nprint(\"\\nüìú Conversation history:\")\nchatbot.show_history()\n\n\n# ============================================================================\n# FEATURE 4: Batch Processing\n# ============================================================================\n\n\"\"\"\nCell 21: Batch Legal Document Processor\n\"\"\"\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef batch_process(questions_list, output_file=\"legal_qa_results.csv\"):\n    \"\"\"Process multiple legal questions in batch\"\"\"\n    \n    results = []\n    \n    for question in tqdm(questions_list, desc=\"Processing queries\"):\n        prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, do_sample=True)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        answer = answer.split(\"### Response:\")[-1].strip()\n        \n        results.append({\n            \"question\": question,\n            \"answer\": answer\n        })\n    \n    # Save to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(output_file, index=False, encoding='utf-8')\n    \n    print(f\"\\n‚úÖ Processed {len(results)} questions\")\n    print(f\"üìÑ Results saved to {output_file}\")\n    \n    return df\n\n# Test batch processing\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing Batch Processor\")\nprint(\"=\"*80)\n\nquestions = [\n    \"What is murder under Nepal law?\",\n    \"What is the punishment for assault?\",\n    \"What is fraud?\",\n]\n\nresults_df = batch_process(questions)\nprint(\"\\n\", results_df)\n\n\n# ============================================================================\n# FEATURE 5: Document Summarizer\n# ============================================================================\n\n\"\"\"\nCell 22: Legal Document Summarization\n\"\"\"\ndef summarize_document(long_text, summary_type=\"brief\"):\n    \"\"\"Summarize long legal documents\"\"\"\n    \n    if summary_type == \"brief\":\n        instruction = \"Provide a brief 2-3 sentence summary of this legal text.\"\n        max_tokens = 150\n    elif summary_type == \"detailed\":\n        instruction = \"Provide a detailed summary of this legal text, including key points.\"\n        max_tokens = 400\n    else:\n        instruction = \"Extract the main points from this legal text.\"\n        max_tokens = 300\n    \n    prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{long_text}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.5, do_sample=True)\n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    summary = summary.split(\"### Response:\")[-1].strip()\n    \n    return summary\n\n# Test summarizer\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing Document Summarizer\")\nprint(\"=\"*80)\n\nlong_legal_text = \"\"\"\nNational Penal Code 2017 Section 177 deals with theft. According to this section,\ntheft is defined as the dishonest taking of movable property belonging to another\nperson without their consent. The punishment for theft includes imprisonment for\na term which may extend to seven years and also fine which may extend to seventy\nthousand rupees. The severity of punishment may vary based on the value of stolen\nproperty and circumstances of the theft. If theft is committed during nighttime\nor in a dwelling house, additional penalties may apply.\n\"\"\"\n\nprint(\"\\nüìÑ Brief Summary:\")\nprint(summarize_document(long_legal_text, \"brief\"))\n\nprint(\"\\nüìÑ Detailed Summary:\")\nprint(summarize_document(long_legal_text, \"detailed\"))\n\n\n# ============================================================================\n# FEATURE 6: Confidence Scoring\n# ============================================================================\n\n\"\"\"\nCell 23: Answer with Confidence Score\n\"\"\"\nimport torch.nn.functional as F\n\ndef answer_with_confidence(question):\n    \"\"\"Generate answer with confidence score\"\"\"\n    \n    prompt = f\"\"\"### Instruction:\nExplain this legal provision in simple language.\n\n### Input:\n{question}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.7,\n            do_sample=True,\n            output_scores=True,\n            return_dict_in_generate=True\n        )\n    \n    # Calculate average confidence\n    scores = outputs.scores\n    confidences = [F.softmax(score, dim=-1).max().item() for score in scores]\n    avg_confidence = sum(confidences) / len(confidences)\n    \n    answer = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    answer = answer.split(\"### Response:\")[-1].strip()\n    \n    return {\n        \"answer\": answer,\n        \"confidence\": avg_confidence,\n        \"confidence_label\": \"High\" if avg_confidence > 0.9 else \"Medium\" if avg_confidence > 0.7 else \"Low\"\n    }\n\n# Test confidence scoring\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing Confidence Scoring\")\nprint(\"=\"*80)\n\nresult = answer_with_confidence(\"What is the punishment for theft?\")\nprint(f\"\\n‚ùì Question: What is the punishment for theft?\")\nprint(f\"\\nüí¨ Answer: {result['answer']}\")\nprint(f\"\\nüìä Confidence: {result['confidence']:.2%} ({result['confidence_label']})\")\n\n\n# ============================================================================\n# SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ ALL ADVANCED FEATURES LOADED SUCCESSFULLY!\")\nprint(\"=\"*80)\nprint(\"\\nüìö Available Features:\")\nprint(\"  1. ‚úÖ RAG System - rag_query(question)\")\nprint(\"  2. ‚úÖ Case Analyzer - analyze_case(description)\")\nprint(\"  3. ‚úÖ Chatbot - chatbot.chat(message)\")\nprint(\"  4. ‚úÖ Batch Processor - batch_process(questions)\")\nprint(\"  5. ‚úÖ Summarizer - summarize_document(text)\")\nprint(\"  6. ‚úÖ Confidence Score - answer_with_confidence(question)\")\nprint(\"\\nüí° Try them out now!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:13:12.562568Z","iopub.status.idle":"2026-01-15T18:13:12.562865Z","shell.execute_reply.started":"2026-01-15T18:13:12.562732Z","shell.execute_reply":"2026-01-15T18:13:12.562747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}